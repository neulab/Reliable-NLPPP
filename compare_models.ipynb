{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "from sklearn import metrics\n",
    "\n",
    "import tensorly as tl\n",
    "import copy\n",
    "from tensorly.decomposition import parafac\n",
    "import random\n",
    "import pickle\n",
    "import sys\n",
    "import itertools\n",
    "import math\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.transforms as mtransforms\n",
    "import csv\n",
    "from sklearn import preprocessing\n",
    "  \n",
    "from tensorly.decomposition import tucker\n",
    "from tensorly import tucker_to_tensor\n",
    "from tensorly.decomposition import robust_pca\n",
    "\n",
    "from sklearn.gaussian_process.kernels import Kernel, Hyperparameter, Matern, WhiteKernel\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR F1 TENSOR\n",
    "\n",
    "def get_dict_f1(path, metric=\"f1\"):\n",
    "    with open(path, \"rb\") as f:\n",
    "        tensorDict_list = pickle.load(f)\n",
    "        if metric == \"f1\":\n",
    "            d = tensorDict_list[0]\n",
    "    return d\n",
    "     \n",
    "\n",
    "def iter_dict_f1(tensor_dict, mode_dict, dept=0):\n",
    "    for k,v in tensor_dict.items():\n",
    "        if isinstance(v,dict):\n",
    "            mode_dict[dept].add(k)\n",
    "            iter_dict_f1(v, mode_dict, dept+1)\n",
    "        else:\n",
    "            mode_dict[dept].add(k)\n",
    "            mode_dict[dept+1].add(v)\n",
    "            \n",
    "    \n",
    "def get_modes_f1(tensorDict, numMode):\n",
    "    # numMode + 1 since values take 1 extra level \n",
    "    modeDict = {i: set() for i in range(numMode+1)} \n",
    "    iter_dict_f1(tensorDict, modeDict)\n",
    "    return modeDict\n",
    "    \n",
    "# assumes ordinal buckets, but there are categorical buckets: e.g (\"('loc',), 'misc',\",'org','per',)?)\n",
    "def order_buckets_f1(d):\n",
    "    d_copy = copy.deepcopy(d)\n",
    "    for model in d.keys():\n",
    "        for dataset in d[model].keys():\n",
    "            for attribute in d[model][dataset].keys():\n",
    "                d_copy[model][dataset][attribute] = {}\n",
    "                for i, (k, v) in enumerate(d[model][dataset][attribute].items()):\n",
    "                    d_copy[model][dataset][attribute][i] = v\n",
    "    return d_copy\n",
    "\n",
    "# for now assumes directions go as: model -> dataset -> attribute -> bucket\n",
    "def get_index_f1(mode_dict):\n",
    "    index_lst = []\n",
    "    for m in mode_dict:\n",
    "        md = dict()\n",
    "        for i, n in enumerate(list(mode_dict[m])):\n",
    "            md[n] = i\n",
    "        index_lst.append(md)\n",
    "    return index_lst\n",
    "\n",
    "# https://stackoverflow.com/questions/14692690/access-nested-dictionary-items-via-a-list-of-keys\n",
    "def read_dict_f1(dataDict, maplist):\n",
    "    first, rest = maplist[0], maplist[1:]\n",
    "\n",
    "    # Handle missing data\n",
    "    if dataDict.get(first) == None:\n",
    "        return None\n",
    "    \n",
    "    if rest: \n",
    "        return read_dict_f1(dataDict[first], rest)\n",
    "    \n",
    "    else:\n",
    "        # return None if key is missing\n",
    "        return dataDict.get(first) \n",
    "\n",
    "    \n",
    "def read_tensor_f1(tensor, maplist):\n",
    "    first, rest = maplist[0], maplist[1:]\n",
    "    if rest: \n",
    "        return read_tensor_f1(tensor[first], rest)\n",
    "    else:\n",
    "        return tensor[first]\n",
    "   \n",
    "    \n",
    "    \n",
    "\n",
    "def write_tensor_f1(tensor, maplist, val):\n",
    "    first, rest = maplist[0], maplist[1:]\n",
    "\n",
    "    if rest: \n",
    "        assert(isinstance(tensor[first], np.ndarray))\n",
    "        return write_tensor_f1(tensor[first], rest, val)\n",
    "    else:\n",
    "        if isinstance(tensor[first], np.ndarray):\n",
    "            print(\"Maplist length not correct!\")\n",
    "        tensor[first] = val\n",
    "    \n",
    "    \n",
    "# Construct a tensor of dimensions *argv    \n",
    "def init_tensor_f1(*argv):\n",
    "    size = 1\n",
    "    for s in argv:\n",
    "        size *= s\n",
    "    X = tl.tensor(np.arange(size).reshape(argv), dtype=tl.float32)\n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "def convert_index_f1(p, indDict):\n",
    "    l = []\n",
    "    for i,n in enumerate(p):\n",
    "        l.append(indDict[i][n])\n",
    "    return l\n",
    "\n",
    "    \n",
    "def construct_tensor_f1(modeDict, tensorDict, indexDict):\n",
    "    l = []\n",
    "    for _,v in modeDict.items():\n",
    "        l.append(list(v))\n",
    "    from_paths = list(itertools.product(*l[:-1])) # all possible entries \n",
    "    \n",
    "    to_paths = []\n",
    "    for p in from_paths:\n",
    "        to_paths.append(convert_index(p, indexDict))\n",
    "  \n",
    "    \n",
    "    # tensor dim\n",
    "    dims = [len(modeDict[i]) for i in modeDict.keys()][:-1]\n",
    "    X = init_tensor_f1(*dims)\n",
    "    \n",
    "    \n",
    "    for p in range(len(from_paths)):\n",
    "       # print(p)\n",
    "        val = read_dict_f1(tensorDict, from_paths[p])\n",
    "        # if data is not missing\n",
    "        if val != None:\n",
    "            write_tensor_f1(X,to_paths[p],val) \n",
    "        else:\n",
    "            print(\"MISSING DATA AT \"+ str(from_paths[p]))\n",
    "            write_tensor_f1(X,to_paths[p],None) \n",
    "    return X\n",
    "        \n",
    "def all_paths_f1(modeDict):\n",
    "    l = []\n",
    "    for _,v in modeDict.items():\n",
    "        l.append(list(v))\n",
    "    p = list(itertools.product(*l[:-1])) # all possible entries \n",
    "    return p\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_df_f1(tensor, mode, index):\n",
    "    all_p = all_paths_f1(mode)\n",
    "    name = [\"features\"]\n",
    "    df = pd.DataFrame(index=range(len(all_p)),columns=name)\n",
    "    f1_list = []\n",
    "    for i in range(len(all_p)):\n",
    "        paste = str(all_p[i])[1:-1]\n",
    "        df.iloc[i,:]['features'] = paste\n",
    "        \n",
    "        \n",
    "        ind = convert_index_f1(all_p[i], index)\n",
    "        f1 = read_tensor_f1(tensor, ind)\n",
    "        f1_list.append(f1)\n",
    "    \n",
    "    df[['model','dataset',\"attribute\",\"bucket\"]] = df['features'].str.split(', ',expand=True)\n",
    "    df = df.drop(['features'], axis=1)\n",
    "    df['f1'] = f1_list\n",
    "    return df\n",
    "\n",
    "def categorize_df_f1(df, ind_lst):\n",
    "    df_new = copy.deepcopy(df)\n",
    "    for i in range(len(df.columns)-2):\n",
    "        col_dict = ind_lst[i]\n",
    "        for j in range(len(df.index)):\n",
    "            entry = df.iloc[j,i]\n",
    "            entry = str(entry)[1:len(entry)-1]\n",
    "            val = col_dict[str(entry)]\n",
    "            df_new.iloc[j,i] = val\n",
    "    df_new['bucket'] = pd.to_numeric(df_new['bucket'])\n",
    "    return df_new\n",
    "        \n",
    "\n",
    "def scale_f1(tensor, mode_dict, index_dict):\n",
    "    all_p = all_paths_f1(mode_dict)\n",
    "    paths = convert_paths(all_p,index_dict)\n",
    "    tensor2 = copy.deepcopy(tensor)\n",
    "    array = tensor.flatten()\n",
    "    m, sd = np.mean(array), np.std(array)\n",
    "    for i in paths:\n",
    "        val = read_tensor_f1(tensor, i)\n",
    "        transformed = (val - m) / sd\n",
    "        write_tensor_f1(tensor2, i, transformed) \n",
    "    return tensor2, m, sd\n",
    "\n",
    "def scale_back_f1(val, mean, sd):\n",
    "#     all_p = all_paths_f1(mode_dict)\n",
    "#     paths = convert_paths(all_p,index_dict)\n",
    "# #     tensor2 = copy.deepcopy(tensor)\n",
    "#     for i in paths:\n",
    " #       val = read_tensor_f1(tensor, i)\n",
    "    scale_back = val * sd + mean\n",
    "  #  write_tensor_f1(tensor2, i, scale_back) \n",
    "    \n",
    "    return scale_back "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_test_split(df, fold=5):\n",
    "    rows = list(range(len(df)))\n",
    "    kf = KFold(n_splits=fold)\n",
    "    \n",
    "    test_lst = []\n",
    "    train_lst = []\n",
    "    for train, test in kf.split(rows):\n",
    "        train_lst.append(train)\n",
    "        test_lst.append(test)\n",
    "        \n",
    "    shuffled_df = df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    folds = []\n",
    "    for i in range(len(test_lst)):\n",
    "        \n",
    "        index_train = list(train_lst[i])\n",
    "        index_test = list(test_lst[i])\n",
    "       # print(\"train i\", index_train)\n",
    "        train = shuffled_df.iloc[index_train,:]\n",
    "        test = shuffled_df.iloc[index_test,:]\n",
    "        folds.append((train, test))\n",
    "    return folds, shuffled_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Wiki\n",
    "\n",
    "def get_modes(df, col_indices):\n",
    "    numModes = len(col_indices)\n",
    "    modeDict = {i: None for i in range(numModes)} \n",
    "    colInd = {j: None for j in col_indices} \n",
    "    for i in modeDict:\n",
    "        c = col_indices[i]\n",
    "        col = df.iloc[:,c]\n",
    "        vals = pd.unique(col)\n",
    "        modeDict[i] = set(vals)\n",
    "        colInd[c] = i\n",
    "    return modeDict, colInd\n",
    "\n",
    "    \n",
    "    modeDict = {i: None for i in col_indices} \n",
    "    for i in col_indices:\n",
    "        col = df.iloc[:,i]\n",
    "        vals = pd.unique(col)\n",
    "        modeDict[i] = set(vals)\n",
    "    return modeDict\n",
    "\n",
    "def get_index(mode_dict):\n",
    "    index_lst = []\n",
    "    for m in mode_dict:\n",
    "        md = dict()\n",
    "        for i, n in enumerate(list(mode_dict[m])):\n",
    "            md[n] = i\n",
    "        index_lst.append(md)\n",
    "    return index_lst\n",
    "\n",
    "\n",
    "def all_paths(mode_dict):\n",
    "    l = []\n",
    "    for _,v in mode_dict.items():\n",
    "        l.append(list(v))\n",
    "    p = list(itertools.product(*l[:])) # all possible entries \n",
    "    return p\n",
    "\n",
    "\n",
    "# Put rows of dataframe to a dict\n",
    "# {(a,b,c,d):BLEU, ...}\n",
    "\n",
    "# Use first 6 cols as modes\n",
    "def df_to_dict(df, key_cols=[0,1,3,4,5], val_col=2):\n",
    "    npdf = np.array(df)\n",
    "    d = {}\n",
    "    for i in range(len(npdf)):\n",
    "        row = npdf[i]\n",
    "        sliced_row= tuple(row[key_cols])\n",
    "       # print(sliced_row)\n",
    "       # print(row[val_col])\n",
    "        if sliced_row in d: \n",
    "            print(\"same key\")\n",
    "        d[sliced_row] = row[val_col]\n",
    "    return d\n",
    "\n",
    "\n",
    "def init_tensor(modeDict):\n",
    "    sl = []\n",
    "    prod = 1\n",
    "    for f in modeDict:\n",
    "        size = len(modeDict[f])\n",
    "        sl.append(size)\n",
    "        prod *= size\n",
    "    t = tl.zeros(sl)\n",
    "    return t   \n",
    "\n",
    "def read_tensor(tensor, maplist):\n",
    "    first, rest = maplist[0], maplist[1:]\n",
    "    if rest: \n",
    "        return read_tensor(tensor[first], rest)\n",
    "    else:\n",
    "        return tensor[first]\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Wiki\n",
    "\n",
    "\n",
    "def scale(df, cols):\n",
    "    new = df.copy()\n",
    "    for i in cols:\n",
    "        new.iloc[:,i] = preprocessing.scale(df.iloc[:,i])\n",
    "    return new\n",
    "\n",
    "def path_to_row(df, source='Source', target='Target'):\n",
    "    path_dict = {}\n",
    "    for i in range(len(df)):\n",
    "        row = df.iloc[i,:]\n",
    "        s, t = row[source], row[target]\n",
    "        path_dict[(s,t)] = i\n",
    "    return path_dict\n",
    "        \n",
    "def get_val(df, path, rows):\n",
    "    s, t, b = path[0], path[1], path[2]\n",
    "    if (s,t) in rows:\n",
    "        r = rows[(s,t)]\n",
    "        val = df.loc[r,b]\n",
    "        return val\n",
    "    else: return None\n",
    "    \n",
    "def convert_index(p, indDict):\n",
    "    l = []\n",
    "    for i,n in enumerate(p):\n",
    "        l.append(indDict[i][n])\n",
    "    return l\n",
    "\n",
    "def write_tensor(tensor, maplist, val):\n",
    "    first, rest = maplist[0], maplist[1:]\n",
    "\n",
    "    if rest: \n",
    "        assert(isinstance(tensor[first], np.ndarray))\n",
    "        return write_tensor(tensor[first], rest, val)\n",
    "    else:\n",
    "        if isinstance(tensor[first], np.ndarray):\n",
    "            print(\"Maplist length not correct!\")\n",
    "        tensor[first] = val\n",
    "    \n",
    "def convert_paths(from_names, indexDict):\n",
    "    to_paths = []\n",
    "    for p in from_names:\n",
    "        to_paths.append(convert_index(p, indexDict))\n",
    "    return to_paths\n",
    "\n",
    "    \n",
    "def fill_tensor(tensor, df, path_lst, index_lst, source='Source', target='Target'):\n",
    "    row_language = path_to_row(df, source=source, target=target)\n",
    "    missing = []\n",
    "    for p in path_lst:\n",
    "        val = get_val(df, p, row_language)\n",
    "        index = convert_index(p, index_lst)\n",
    "        # If the path exists and value is not missing \n",
    "        if val != None and not math.isnan(val):\n",
    "            write_tensor(tensor, index, val)\n",
    "        else:\n",
    "            write_tensor(tensor, index, 0)\n",
    "            missing.append(p)\n",
    "    return tensor, missing\n",
    "\n",
    "                    \n",
    "def create_mask(missing, tensor):\n",
    "    shape = tensor.shape\n",
    "    mask = tl.ones(shape, dtype=tl.float32)\n",
    "    for r in missing:\n",
    "        write_tensor(mask, r,0)\n",
    "    return mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct(tensor, mask_tensor, decomp ='pca'):\n",
    "\n",
    "    if decomp == 'pca':\n",
    "        masked_tensor = tensor * mask_tensor\n",
    "        pca_res = robust_pca(masked_tensor, mask=mask_tensor)\n",
    "        reconstructed = pca_res[0]\n",
    "        \n",
    "    if decomp == 'tucker_decomp': \n",
    "        masked_tensor = tensor * mask_tensor\n",
    "        core, factors = tucker(masked_tensor, rank = [39,44,22])\n",
    "        reconstructed = tucker_to_tensor((core, factors))\n",
    "\n",
    "    if decomp == 'cp':\n",
    "        masked_tensor = tensor * mask_tensor\n",
    "        (w, f), err = parafac(masked_tensor, rank=5, mask = mask_tensor, return_errors = True)\n",
    "        reconstructed = tl.kruskal_to_tensor((w, f))\n",
    "    \n",
    "    return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_val(ind_list, tensor, s, t, f):\n",
    "    s, t, f = ind_list[0][s], ind_list[1][t], ind_list[2][f]\n",
    "    return tensor[s][t][f]\n",
    "\n",
    "def transform(orig_col, val):\n",
    "    m, sd = orig_col.mean(), orig_col.std()\n",
    "    orig = val * sd + m\n",
    "    return orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_tensor_from_df(df, lang_modes=[0,1], need_scale=True,\\\n",
    "                            source='Source', target='Target', feat_mode=None): \n",
    "    \n",
    "    # Get first 2 lang modes\n",
    "    modeDict, colInd = get_modes(df, lang_modes)\n",
    "    \n",
    "    \n",
    "    # Specify axis 3\n",
    "    buckets = df.columns[feat_mode]\n",
    "    \n",
    "\n",
    "    modeDict[2] = set(buckets)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    # Get all possible indices for the tensor\n",
    "    all_p = all_paths(modeDict)\n",
    "    indLst = get_index(modeDict)\n",
    "    \n",
    "    # Init empty tensors\n",
    "    t = init_tensor(modeDict)\n",
    "    \n",
    "    if need_scale:\n",
    "        # Scaled version\n",
    "        df2 = scale(df, [feat_mode])\n",
    "    else:\n",
    "        df2 = copy.deepcopy(df) \n",
    "        \n",
    "    # Create a tensor\n",
    "    t_bleu, missing = fill_tensor(t, df2, all_p, indLst, source=source, target=target)\n",
    "    \n",
    "    # Create a mask\n",
    "   #missing_paths = convert_paths(missing, indLst)\n",
    "   # mask = create_mask(missing_paths, t_bleu)\n",
    "\n",
    "    return t_bleu, missing, modeDict, indLst, all_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_out_test_y(test_y_df, missing, tensor, index_dict, data):\n",
    "    new_missing = copy.deepcopy(missing)\n",
    "    new_tensor = copy.deepcopy(tensor)\n",
    "    test_entries = []\n",
    "    # Mask out BLEUs for test entries\n",
    "    for index, row in test_y_df.iterrows():\n",
    "        if data == 'wiki':\n",
    "            source, target, bleu = row['Source'], row['Target'], 'BLEU'\n",
    "            \n",
    "        if data == 'tsf':\n",
    "            source, target, bleu = row['Task lang'], row['Aux lang'], 'Accuracy'\n",
    "            \n",
    "        if data == 'tsfmt':\n",
    "            source, target, bleu = row[' Source lang'], row['Transfer lang'], 'BLEU'\n",
    "            \n",
    "       # val = row['BLEU']\n",
    "        path = (source, target, bleu)\n",
    "        new_missing.append(path)\n",
    "        test_entries.append(path)\n",
    "        \n",
    "    missing_combined = convert_paths(new_missing, index_dict)\n",
    "    new_mask = create_mask(missing_combined, tensor)\n",
    "    return new_mask, test_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_out_test_y_f1(test_y_df, missing, tensor, index_dict):\n",
    "    new_missing = copy.deepcopy(missing)\n",
    "    new_tensor = copy.deepcopy(tensor)\n",
    "    test_entries = []\n",
    "    # Mask out f1 for test entries\n",
    "    for index, row in test_y_df.iterrows():\n",
    "        mod, dat, attr, buck = row['model'][1:-1], row['dataset'][1:-1], row['attribute'][1:-1], row['bucket']\n",
    "        path = (mod, dat, attr, int(buck))\n",
    "        #print(path)\n",
    "        new_missing.append(path)\n",
    "        test_entries.append(path)\n",
    "        \n",
    "    missing_combined = convert_paths(new_missing, index_dict)\n",
    "    new_mask = create_mask(missing_combined, tensor)\n",
    "    return new_mask, test_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_out_test_y_cws(test_y_df, missing, tensor, index_dict):\n",
    "    new_missing = copy.deepcopy(missing)\n",
    "    new_tensor = copy.deepcopy(tensor)\n",
    "    test_entries = []\n",
    "    # Mask out f1 for test entries\n",
    "    for index, row in test_y_df.iterrows():\n",
    "        dat, mod, attr, buck = row['dataset'][1:-1], row['model'][1:-1], row['attribute'][1:-1], row['bucket']\n",
    "        path = (dat, mod, attr, int(buck))\n",
    "        #print(path)\n",
    "        new_missing.append(path)\n",
    "        test_entries.append(path)\n",
    "        \n",
    "    missing_combined = convert_paths(new_missing, index_dict)\n",
    "    new_mask = create_mask(missing_combined, tensor)\n",
    "    return new_mask, test_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_df_cws(tensor, mode, index):\n",
    "    all_p = all_paths_f1(mode)\n",
    "    name = [\"features\"]\n",
    "    df = pd.DataFrame(index=range(len(all_p)),columns=name)\n",
    "    f1_list = []\n",
    "    for i in range(len(all_p)):\n",
    "        paste = str(all_p[i])[1:-1]\n",
    "        df.iloc[i,:]['features'] = paste\n",
    "        \n",
    "        \n",
    "        ind = convert_index_f1(all_p[i], index)\n",
    "        f1 = read_tensor_f1(tensor, ind)\n",
    "        f1_list.append(f1)\n",
    "    \n",
    "    df[['dataset','model',\"attribute\",\"bucket\"]] = df['features'].str.split(', ',expand=True)\n",
    "    df = df.drop(['features'], axis=1)\n",
    "    df['f1'] = f1_list\n",
    "    return df\n",
    "\n",
    "def categorize_df_cws(df, ind_lst):\n",
    "    df_new = copy.deepcopy(df)\n",
    "    for i in range(len(df.columns)-2):\n",
    "        col_dict = ind_lst[i]\n",
    "        for j in range(len(df.index)):\n",
    "            entry = df.iloc[j,i]\n",
    "            entry = str(entry)[1:len(entry)-1]\n",
    "            val = col_dict[str(entry)]\n",
    "            df_new.iloc[j,i] = val\n",
    "    df_new['bucket'] = pd.to_numeric(df_new['bucket'])\n",
    "    return df_new\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct F1 Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(df, tensor, missing, index_lst,\\\n",
    "                   fold, scale_back=True, \\\n",
    "                   cp_rank=3, data=\"wiki\", \\\n",
    "                   non_scaled_tensor=None,\\\n",
    "                  non_scaled_mode_dict=None,\\\n",
    "                  non_scaled_index=None,\\\n",
    "                  reg_E=1, reg_J=1):\n",
    "    \n",
    "    print(\"fold\",fold)\n",
    "    folds, shulffled = train_test_split(df, fold)\n",
    "    pca_rmse_lst = []\n",
    "    cp_rmse_lst = []\n",
    "    xbg_rmse_lst = []\n",
    "    gp_rmse_lst = []\n",
    "    lg_rmse_lst = []\n",
    "    bs_rmse_lst = []\n",
    "    \n",
    "    for i in range(len(folds)):\n",
    "    \n",
    "    \n",
    "        train, test = folds[i][0], folds[i][1]\n",
    "\n",
    "        # tensor pca\n",
    "        if data == 'wiki' or data == 'tsf' or data == 'tsfmt':\n",
    "            combined_mask, test_missing_entries = mask_out_test_y(test, missing, tensor, index_lst, data)\n",
    "            \n",
    "        if data == 'f1':\n",
    "            combined_mask, test_missing_entries = mask_out_test_y_f1(test, missing, tensor, index_lst)\n",
    "            \n",
    "        if data == 'cws':\n",
    "            combined_mask, test_missing_entries = mask_out_test_y_cws(test, missing, tensor, index_lst)\n",
    "\n",
    "        masked_tensor = tensor * combined_mask\n",
    "#         pca_res = robust_pca(masked_tensor, mask=combined_mask, \\\n",
    "#                              n_iter_max=200, reg_E=0.6, reg_J = 1.65,\\\n",
    "#                              learning_rate=1.1)\n",
    "        \n",
    "        pca_res = robust_pca(masked_tensor, mask=combined_mask, reg_E=reg_E, reg_J=reg_J)\n",
    "        \n",
    "        \n",
    "        # Use only the low rank part, see http://jeankossaifi.com/blog/rpca.html\n",
    "        pca_reconstructed = pca_res[0]\n",
    "     #   pca_reconstructed = pca_res[0] + pca_res[1]\n",
    "\n",
    "\n",
    "        tensor_pred_pca = []\n",
    "        tensor_pred_cp = []\n",
    "        true_y = []\n",
    "        \n",
    "        \n",
    "        # tensor cp decomp\n",
    "        \n",
    "        (w, f), err = parafac(masked_tensor, rank=cp_rank, mask = combined_mask, return_errors = True)\n",
    "        cp_reconstructed = tl.kruskal_to_tensor((w, f))\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        for m in test_missing_entries:\n",
    "\n",
    "#             if m == ('eng', 'fra', 'BLEU'):\n",
    "#                 print(\"!!!!!\")\n",
    "#                 temp = convert_index(m,index_lst)\n",
    "#                 pr = read_tensor(pca_reconstructed, temp)\n",
    "#                 tr= read_tensor(tensor, temp)\n",
    "#                 print(\"Tr,\", tr)\n",
    "#                 print(\"pr,\", pr)\n",
    "                \n",
    "            blank = convert_index(m, index_lst)\n",
    "            \n",
    "            # PCA\n",
    "            pred_pca = read_tensor(pca_reconstructed, blank)\n",
    "            \n",
    "        \n",
    "            # CP\n",
    "            pred_cp = read_tensor(cp_reconstructed, blank)\n",
    "            \n",
    "            \n",
    "            true = read_tensor(tensor, blank)\n",
    "            \n",
    "          #  print(\"pred pca\", pred_pca)\n",
    "         #   print(\"true pca\", true)\n",
    "\n",
    "            # if tensor has been standardized \n",
    "            if scale_back:\n",
    "                \n",
    "                \n",
    "                if data == 'wiki':\n",
    "                    pred_pca = transform(df.loc[:,'BLEU'], pred_pca)\n",
    "                   # pred_pca = transform(train.loc[:,'BLEU'], pred_pca)\n",
    "\n",
    "                    pred_cp = transform(df.loc[:,'BLEU'], pred_cp)\n",
    "                   # pred_cp = transform(train.loc[:,'BLEU'], pred_cp)\n",
    "\n",
    "                    true = transform(df.loc[:,'BLEU'], true)\n",
    "                   # true = transform(train.loc[:,'BLEU'], true)\n",
    "                \n",
    "                if data == 'tsf':\n",
    "                    pred_pca = transform(df.loc[:,'Accuracy'], pred_pca)\n",
    "                   # pred_pca = transform(train.loc[:,'BLEU'], pred_pca)\n",
    "\n",
    "                    pred_cp = transform(df.loc[:,'Accuracy'], pred_cp)\n",
    "                   # pred_cp = transform(train.loc[:,'BLEU'], pred_cp)\n",
    "\n",
    "                    true = transform(df.loc[:,'Accuracy'], true)\n",
    "                   # true = transform(train.loc[:,'BLEU'], true)\n",
    "                \n",
    "                if data == 'tsfmt':\n",
    "                    pred_pca = transform(df.loc[:,'BLEU'], pred_pca)\n",
    "                   # pred_pca = transform(train.loc[:,'BLEU'], pred_pca)\n",
    "\n",
    "                    pred_cp = transform(df.loc[:,'BLEU'], pred_cp)\n",
    "                   # pred_cp = transform(train.loc[:,'BLEU'], pred_cp)\n",
    "\n",
    "                    true = transform(df.loc[:,'BLEU'], true)\n",
    "                   # true = transform(train.loc[:,'BLEU'], true)\n",
    "                \n",
    "                \n",
    "                \n",
    "                if data == 'f1':\n",
    "                    _, m, sd = scale_f1(non_scaled_tensor, non_scaled_mode_dict, non_scaled_index)\n",
    "                    pred_pca = scale_back_f1(pred_pca, m, sd)\n",
    "                    pred_cp = scale_back_f1(pred_cp, m, sd)\n",
    "                    true = scale_back_f1(true, m, sd)\n",
    "                    \n",
    "                    \n",
    "            tensor_pred_pca.append(pred_pca)\n",
    "            tensor_pred_cp.append(pred_cp)\n",
    "            \n",
    "            true_y.append(true)\n",
    "\n",
    "\n",
    "        # xgboost\n",
    "\n",
    "        if data == 'wiki' or data == 'tsf' or data == 'tsfmt':\n",
    "        \n",
    "            fold_train_X = train.iloc[:,3:]\n",
    "            fold_train_y = train.iloc[:,2]\n",
    "\n",
    "            fold_test_X = test.iloc[:,3:]\n",
    "            fold_test_y = test.iloc[:,2]\n",
    "            \n",
    "            \n",
    "            \n",
    "        if data == 'f1' or data == 'cws':\n",
    "            \n",
    "            \n",
    "            train = categorize_df_f1(train, index_lst)\n",
    "            test = categorize_df_f1(test, index_lst)\n",
    "        \n",
    "                \n",
    "            fold_train_X = train.iloc[:,:-1]\n",
    "            fold_train_y = train.iloc[:,-1]\n",
    "            \n",
    "            fold_test_X = test.iloc[:,:-1]\n",
    "            fold_test_y = test.iloc[:,-1]\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        reg = xgb.XGBRegressor(objective ='reg:squarederror', learning_rate=0.1,\n",
    "                                   max_depth=10, n_estimators=100)\n",
    "        res = reg.fit(fold_train_X, fold_train_y)\n",
    "\n",
    "        xgb_pred_y = res.predict(fold_test_X)\n",
    "        \n",
    "        #### Add gp #####\n",
    "        \n",
    "        ker = Matern()# + WhiteKernel()\n",
    "        gpr = GaussianProcessRegressor(kernel=ker).fit(fold_train_X, fold_train_y)\n",
    "        gp_pred_y = gpr.predict(fold_test_X)\n",
    "        \n",
    "        #######\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ### Add LGBM ####\n",
    "            \n",
    "        lg = lgb.LGBMRegressor(objective='regression')#, num_leaves=120, learning_rate=0.08, \\\n",
    "                                  # n_estimators=30, max_depth = 5)\n",
    "        lg.fit(fold_train_X, fold_train_y, verbose=False)\n",
    "        lg_pred_y = lg.predict(fold_test_X)\n",
    "        ### Added LGBM ####\n",
    "        \n",
    "        \n",
    "        ### Add Baseline\n",
    "        mean_y = np.mean(fold_train_y)\n",
    "        base_pred_y = [mean_y]*len(lg_pred_y)\n",
    "        #######\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        rmse_pca = np.sqrt(mean_squared_error(tensor_pred_pca, true_y))\n",
    "        rmse_cp = np.sqrt(mean_squared_error(tensor_pred_cp, true_y))\n",
    "        rmse_xgb = np.sqrt(mean_squared_error(xgb_pred_y, fold_test_y))\n",
    "        ### Add gp\n",
    "        rmse_gp = np.sqrt(mean_squared_error(gp_pred_y, fold_test_y))\n",
    "        ### Add lgbm\n",
    "        rmse_lg = np.sqrt(mean_squared_error(lg_pred_y, fold_test_y))\n",
    "        ### Add baseline\n",
    "        rmse_bs = np.sqrt(mean_squared_error(base_pred_y, fold_test_y))\n",
    "        \n",
    "        \n",
    "        \n",
    "        pca_rmse_lst.append(rmse_pca)\n",
    "        cp_rmse_lst.append(rmse_cp)\n",
    "        xbg_rmse_lst.append(rmse_xgb)\n",
    "        \n",
    "        ### Add gp\n",
    "        gp_rmse_lst.append(rmse_gp)\n",
    "        ### Add lgbm\n",
    "        lg_rmse_lst.append(rmse_lg)\n",
    "        ### Add baseline\n",
    "        bs_rmse_lst.append(rmse_bs)\n",
    "        \n",
    "        \n",
    "    return pca_rmse_lst, cp_rmse_lst, xbg_rmse_lst, gp_rmse_lst, lg_rmse_lst, bs_rmse_lst\n",
    " #   return pca_reconstructed, tensor_pred, true_y, xgb_pred_y, fold_test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CbertWnon_snonMlp': 0, 'CcnnWglove_cnnCrf': 1, 'CcnnWnone_lstmCrf': 2, 'CflairWglove_lstmCrf': 3, 'CelmoWnone_lstmCrf': 4, 'CcnnWglove_lstmCrf': 5, 'CcnnWglove_lstmMlp': 6, 'CcnnWrand_lstmCrf': 7, 'CflairWnone_lstmCrf': 8, 'CnoneWrand_lstmCrf': 9, 'CelmoWglove_lstmCrf': 10}\n",
      "{'notebc': 0, 'conll03': 1, 'notebn': 2, 'notewb': 3, 'wnut16': 4, 'notemz': 5}\n",
      "{'oDen': 0, 'eDen': 1, 'eLen': 2, 'eFre': 3, 'sLen': 4, 'eCon': 5, 'tag': 6, 'tCon': 7, 'tFre': 8}\n",
      "{0: 0, 1: 1, 2: 2, 3: 3}\n"
     ]
    }
   ],
   "source": [
    "# Read in a dictionary \n",
    "fileName = \"model_data_metric_bucket_evals6.pkl\"\n",
    "dict_f1 = get_dict_f1(fileName)\n",
    "\n",
    "dict_f1 = order_buckets_f1(dict_f1)\n",
    "dict_f1\n",
    "\n",
    "\n",
    "# Find out all unique feature names and define the 4 dimensions for a tensor\n",
    "# 4 modes: 0: models, 1: datasets, 2: attributes, 3: buckets\n",
    "mdict = get_modes_f1(dict_f1, 4)\n",
    "mdict\n",
    "\n",
    "# Get a mapping between all unique feature names and tensor indices\n",
    "index_mode = get_index_f1(mdict)\n",
    "\n",
    "# Size of the tensor: numModel*numData*numAttribute*numBucket = 11*6*9*4\n",
    "print(index_mode[0])\n",
    "print(index_mode[1])\n",
    "print(index_mode[2])\n",
    "print(index_mode[3])\n",
    "\n",
    "# Construct a tensor with a mode dict, the original tensor dict, and mapping between mode and indices\n",
    "t1 = construct_tensor_f1(mdict, dict_f1,index_mode)\n",
    "t1\n",
    "\n",
    "# convert f1 tensor to a dataframe\n",
    "f1_df = convert_to_df_f1(t1, mdict, index_mode)\n",
    "\n",
    "# convert columns to integers\n",
    "#int_f1_df = categorical_to_int(f1_df)\n",
    "int_f1_df = categorize_df_f1(f1_df, index_mode)\n",
    "\n",
    "# save the table\n",
    "#int_f1_df.to_csv('f1_table.csv') \n",
    "\n",
    "# Specify features X and y\n",
    "#X = f1_df[['model','dataset',\"attribute\",\"bucket\"]]\n",
    "#y = f1_df['f1']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 6, 9, 4)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # xgboost\n",
    "\n",
    "# f1X = int_f1_df.iloc[:,:-1]\n",
    "# f1y = int_f1_df.iloc[:,-1]\n",
    "\n",
    "# #fold_test_X = test.iloc[:,3:]\n",
    "# #fold_test_y = test.iloc[:,2]\n",
    "\n",
    "# reg_f1 = xgb.XGBRegressor(objective ='reg:squarederror', learning_rate=0.1,\n",
    "#                            max_depth=10, n_estimators=100)\n",
    "# res_f1 = reg_f1.fit(f1X, f1y)\n",
    "\n",
    "# xgb_pred_y = res_f1.predict(f1X)\n",
    "# xgb_pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER F1\n",
    "### not scaled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-32955baa086f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcompare_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'f1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscale_back\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreg_E\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_J\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-103-927d296f15b9>\u001b[0m in \u001b[0;36mcompare_models\u001b[0;34m(df, tensor, missing, index_lst, fold, scale_back, cp_rank, data, non_scaled_tensor, non_scaled_mode_dict, non_scaled_index, reg_E, reg_J)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMatern\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# + WhiteKernel()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mgpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussianProcessRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold_train_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold_train_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0mgp_pred_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold_test_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/gaussian_process/gpr.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    219\u001b[0m             optima = [(self._constrained_optimization(obj_func,\n\u001b[1;32m    220\u001b[0m                                                       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m                                                       self.kernel_.bounds))]\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;31m# Additional runs are performed from log-uniform chosen initial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/gaussian_process/gpr.py\u001b[0m in \u001b[0;36m_constrained_optimization\u001b[0;34m(self, obj_func, initial_theta, bounds)\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"fmin_l_bfgs_b\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mtheta_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvergence_dict\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m                 \u001b[0mfmin_l_bfgs_b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_theta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconvergence_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"warnflag\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m                 warnings.warn(\"fmin_l_bfgs_b terminated abnormally with the \"\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfmin_l_bfgs_b\u001b[0;34m(func, x0, fprime, args, approx_grad, bounds, m, factr, pgtol, epsilon, iprint, maxfun, maxiter, disp, callback, maxls)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     res = _minimize_lbfgsb(fun, x0, args=args, jac=jac, bounds=bounds,\n\u001b[0;32m--> 199\u001b[0;31m                            **opts)\n\u001b[0m\u001b[1;32m    200\u001b[0m     d = {'grad': res['jac'],\n\u001b[1;32m    201\u001b[0m          \u001b[0;34m'task'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/gaussian_process/gpr.py\u001b[0m in \u001b[0;36mobj_func\u001b[0;34m(theta, eval_gradient)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0meval_gradient\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                     lml, grad = self.log_marginal_likelihood(\n\u001b[0;32m--> 213\u001b[0;31m                         theta, eval_gradient=True)\n\u001b[0m\u001b[1;32m    214\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/gaussian_process/gpr.py\u001b[0m in \u001b[0;36mlog_marginal_likelihood\u001b[0;34m(self, theta, eval_gradient)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0meval_gradient\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_gradient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/gaussian_process/kernels.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, Y, eval_gradient)\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnu\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1.5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m                 \u001b[0mK_gradient\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1376\u001b[0;31m                     \u001b[0;36m3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mD\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1377\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnu\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2.5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m                 \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "compare_models(f1_df, t1, [],index_mode, 5, data='f1',scale_back=False,reg_E=1, reg_J=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 5\n"
     ]
    }
   ],
   "source": [
    "#ns_gp_result_f1\n",
    "ns_pca_result_f1, ns_cp_result_f1, ns_xgb_result_f1, ns_gp_result_f1,ns_lg_result_f1, ns_bs_result_f1 = compare_models(f1_df, t1, [], \\\n",
    "                                                                     index_mode, 5, data='f1',\\\n",
    "                                                                     scale_back=False,\\\n",
    "                                                                     reg_E=1, reg_J=1\\\n",
    "                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.052196927"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(ns_pca_result_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06890764"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(ns_cp_result_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05419675368129184"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(ns_xgb_result_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07680016503647537"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(ns_gp_result_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05886706942409233"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(ns_lg_result_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20871759077991756"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(ns_bs_result_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 5\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Singleton array array(5) cannot be considered a valid collection.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-42a787a38f6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                             \u001b[0mnon_scaled_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                             \u001b[0mnon_scaled_mode_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmdict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                                                             \u001b[0mnon_scaled_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m                                                            )\n",
      "\u001b[0;32m<ipython-input-91-927d296f15b9>\u001b[0m in \u001b[0;36mcompare_models\u001b[0;34m(df, tensor, missing, index_lst, fold, scale_back, cp_rank, data, non_scaled_tensor, non_scaled_mode_dict, non_scaled_index, reg_E, reg_J)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fold\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mfolds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshulffled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mpca_rmse_lst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mcp_rmse_lst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   2094\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid parameters passed: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2095\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2096\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2098\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \"\"\"\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \"\"\"\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             raise TypeError(\"Singleton array %r cannot be considered\"\n\u001b[0;32m--> 146\u001b[0;31m                             \" a valid collection.\" % x)\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0;31m# Check that shape is returning an integer or default to len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;31m# Dask dataframes may not return numeric shape[0] value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Singleton array array(5) cannot be considered a valid collection."
     ]
    }
   ],
   "source": [
    "scaled_t1,_,_ = scale_f1(t1, mdict, index_mode)\n",
    "s_pca_result_f1, s_cp_result_f1, s_xgb_result_f1, s_gp_result_f1 = compare_models(f1_df, scaled_t1, [], index_mode, 5, data='f1', \\\n",
    "                                                            scale_back=True, \\\n",
    "                                                            non_scaled_tensor=t1,\n",
    "                                                            non_scaled_mode_dict=mdict,\\\n",
    "                                                            non_scaled_index=index_mode\n",
    "                                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaled_t1 = scale_f1(t1, mdict, index_mode)\n",
    "#scaled_t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.058243893"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(s_pca_result_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06304242"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(s_cp_result_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 's_xgb_result_f1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-e394ab8526ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_xgb_result_f1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 's_xgb_result_f1' is not defined"
     ]
    }
   ],
   "source": [
    "np.mean(s_xgb_result_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 's_gp_result_f1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-444db7c8c551>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_gp_result_f1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 's_gp_result_f1' is not defined"
     ]
    }
   ],
   "source": [
    "np.mean(s_gp_result_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_t1,mm,ss= scale_f1(t1, mdict, index_mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wiki MT\n",
    "### Compare XGboost with Tensor PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in wiki data\n",
    "with open(\"dict_file.pkl\", \"rb\") as f:\n",
    "    wkdata = pickle.load(f)\n",
    "    \n",
    "X = wkdata['BLEU']['feats']\n",
    "y = wkdata['BLEU']['labels']\n",
    "langs = wkdata['BLEU']['langs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain a complete dataframe\n",
    "wiki = pd.concat([langs, y, X], axis=1)\n",
    "#wiki.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a tensor with 3 modes, scaled \n",
    "tensor, missing, mode_dict, ind_lst, all_p = construct_tensor_from_df(wiki, lang_modes=[0,1], \\\n",
    "                                           feat_mode=list(range(2,len(wiki.columns))), \\\n",
    "                                           need_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensor\n",
    "#len(ind_lst[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3458251150558843"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sparcity\n",
    "len(missing)/tensor.size\n",
    "#tensor.size\n",
    "#tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do not forget to set REG_E, REG_J terms !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# DO NOT FORGET TO SET THE REG_E, REG_J terms !!!!!!\n",
    "pca_result, cp_result, xgb_result, gp_result, lg_result, bs_result = compare_models(wiki, tensor, missing, ind_lst, 5, reg_E=0.6, reg_J=1.65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.5065002847396687,\n",
       " 2.0757556740884007,\n",
       " 2.6479330137907273,\n",
       " 2.9549646410347155,\n",
       " 3.4725733117494366]"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.5633073504571255,\n",
       " 3.2495450978397593,\n",
       " 3.6346073224477298,\n",
       " 4.12238532160684,\n",
       " 4.793139240513713]"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.59784630274561,\n",
       " 2.141474952802471,\n",
       " 2.994903010038946,\n",
       " 2.3822415590608292,\n",
       " 2.6302115501713765]"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9315453850805895"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(pca_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.072596866573034"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cp_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5493354749638466"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(xgb_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.614821602520268"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(gp_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2876414815595645"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(lg_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.38012240548762"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(bs_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tsf pos\n",
    "### Compare XGboost with Tensor PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in tsf data\n",
    "with open(\"dict_file_tsf.pkl\", \"rb\") as f:\n",
    "    tsfdata = pickle.load(f)\n",
    "    \n",
    "X_tsf = tsfdata['Accuracy']['feats']\n",
    "y_tsf = tsfdata['Accuracy']['labels']\n",
    "langs_tsf = tsfdata['Accuracy']['langs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tsf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain a complete dataframe\n",
    "tsf = pd.concat([langs_tsf, y_tsf, X_tsf], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a tensor with 3 modes, scaled \n",
    "tensor_tsf, missing_tsf, mode_dict_tsf, ind_lst_tsf, all_p_tsf = construct_tensor_from_df(tsf, lang_modes=[0,1], \\\n",
    "                                           feat_mode=list(range(2,len(tsf.columns))),\\\n",
    "                                           need_scale=False, source='Task lang', target='Aux lang')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01858974358974359"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Sparcity\n",
    "len(missing_tsf)/tensor_tsf.size\n",
    "#tensor.size\n",
    "#len(missing_tsf)\n",
    "#missing_tsf\n",
    "#tensor_tsf.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensor_tsf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do not forget to set REG_E, REG_J terms !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# DO NOT FORGET TO SET THE REG_E, REG_J terms !!!!!!\n",
    "pca_result_tsf, cp_result_tsf, xgb_result_tsf, gp_result_tsf,lg_result_tsf, bs_result_tsf = compare_models(tsf, tensor_tsf, missing_tsf, ind_lst_tsf, 5, \\\n",
    "                                                              data='tsf', scale_back=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.2992236791204,\n",
       " 5.220995138959067,\n",
       " 6.244504347252348,\n",
       " 5.309591453857032,\n",
       " 5.661249683280757]"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_result_tsf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[35.75320383101946,\n",
       " 35.054940952357185,\n",
       " 34.06411820079632,\n",
       " 35.09605539789433,\n",
       " 34.10117319297286]"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp_result_tsf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8.372023745611038,\n",
       " 6.33853626718467,\n",
       " 8.747223800968516,\n",
       " 6.311569132593977,\n",
       " 7.227456793547817]"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_result_tsf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.747112860493921"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(pca_result_tsf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34.81389831500803"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cp_result_tsf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.399361947981204"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(xgb_result_tsf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56.5308513830819"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(gp_result_tsf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.050069030751123"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(lg_result_tsf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.10018021212573"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(bs_result_tsf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tsf mt\n",
    "### Compare XGboost with Tensor PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in tsf data\n",
    "with open(\"dict_file_tsfmt.pkl\", \"rb\") as f:\n",
    "    tsfmt = pickle.load(f)\n",
    "    \n",
    "X_tsfmt = tsfmt['BLEU']['feats']\n",
    "y_tsfmt = tsfmt['BLEU']['labels']\n",
    "langs_tsfmt = tsfmt['BLEU']['langs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tsfmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain a complete dataframe\n",
    "tsfmt = pd.concat([langs_tsfmt, y_tsfmt, X_tsfmt], axis=1)\n",
    "tsfmt = tsfmt.drop(columns=['Target lang'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a tensor with 3 modes, scaled \n",
    "tensor_tsfmt, missing_tsfmt, mode_dict_tsfmt, ind_lst_tsfmt, all_p_tsfmt = construct_tensor_from_df(tsfmt, \\\n",
    "                                           lang_modes=[0,1], \\\n",
    "                                           feat_mode=list(range(2,len(tsfmt.columns))),\\\n",
    "                                           need_scale=False, source=' Source lang', target='Transfer lang')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.018518518518518517"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(missing_tsfmt)/tensor_tsfmt.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 54, 21)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_tsfmt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# DO NOT FORGET TO SET THE REG_E, REG_J terms !!!!!!\n",
    "pca_result_tsfmt, cp_result_tsfmt, xgb_result_tsfmt, gp_result_tsfmt, lg_result_tsfmt, bs_result_tsfmt = compare_models(tsfmt, tensor_tsfmt, missing_tsfmt, ind_lst_tsfmt, 5, \\\n",
    "                                                              data='tsfmt', scale_back=False, reg_E=0.6, reg_J=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.5382860382122203,\n",
       " 1.3828968519866267,\n",
       " 0.8663211394403862,\n",
       " 1.4093513545570078,\n",
       " 1.6031248685127348]"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_result_tsfmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8.404722935871503,\n",
       " 8.505859801163583,\n",
       " 8.869100714358991,\n",
       " 8.570899948944483,\n",
       " 8.978368776632799]"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp_result_tsfmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.5414878818578572,\n",
       " 1.455519084439652,\n",
       " 0.9791849288407051,\n",
       " 1.5129307068516908,\n",
       " 1.7787268568990355]"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_result_tsfmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3599960505417952"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(pca_result_tsfmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.665790435394273"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cp_result_tsfmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4535698917777882"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(xgb_result_tsfmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.865616524712625"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(gp_result_tsfmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3073909851934349"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(lg_result_tsfmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.770827148260167"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(bs_result_tsfmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in a dictionary \n",
    "true_cws =  \"true_cws_dict.pkl\"\n",
    "with open(true_cws, \"rb\") as f:\n",
    "    cws_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "cws_modes = get_modes_f1(cws_dict, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "cws_index = get_index_f1(cws_modes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "cws_t = construct_tensor_f1(cws_modes, cws_dict, cws_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert f1 tensor to a dataframe\n",
    "cws_df = convert_to_df_cws(cws_t, cws_modes, cws_index)\n",
    "\n",
    "# convert columns to integers\n",
    "#int_f1_df = categorical_to_int(f1_df)\n",
    "int_cws_df = categorize_df_cws(cws_df, cws_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>attribute</th>\n",
       "      <th>bucket</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'sxu'</td>\n",
       "      <td>'Cw2vBw2vLstmCrf'</td>\n",
       "      <td>'oDen'</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'sxu'</td>\n",
       "      <td>'Cw2vBw2vLstmCrf'</td>\n",
       "      <td>'oDen'</td>\n",
       "      <td>1</td>\n",
       "      <td>0.941879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'sxu'</td>\n",
       "      <td>'Cw2vBw2vLstmCrf'</td>\n",
       "      <td>'oDen'</td>\n",
       "      <td>2</td>\n",
       "      <td>0.929166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'sxu'</td>\n",
       "      <td>'Cw2vBw2vLstmCrf'</td>\n",
       "      <td>'oDen'</td>\n",
       "      <td>3</td>\n",
       "      <td>0.934691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'sxu'</td>\n",
       "      <td>'Cw2vBw2vLstmCrf'</td>\n",
       "      <td>'cFre'</td>\n",
       "      <td>0</td>\n",
       "      <td>0.876042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>'sxu'</td>\n",
       "      <td>'Cw2vBw2vLstmCrf'</td>\n",
       "      <td>'cFre'</td>\n",
       "      <td>1</td>\n",
       "      <td>0.891689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>'sxu'</td>\n",
       "      <td>'Cw2vBw2vLstmCrf'</td>\n",
       "      <td>'cFre'</td>\n",
       "      <td>2</td>\n",
       "      <td>0.940681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>'sxu'</td>\n",
       "      <td>'Cw2vBw2vLstmCrf'</td>\n",
       "      <td>'cFre'</td>\n",
       "      <td>3</td>\n",
       "      <td>0.972888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>'sxu'</td>\n",
       "      <td>'Cw2vBw2vLstmCrf'</td>\n",
       "      <td>'wFre'</td>\n",
       "      <td>0</td>\n",
       "      <td>0.849907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>'sxu'</td>\n",
       "      <td>'Cw2vBw2vLstmCrf'</td>\n",
       "      <td>'wFre'</td>\n",
       "      <td>1</td>\n",
       "      <td>0.814121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>'sxu'</td>\n",
       "      <td>'Cw2vBw2vLstmCrf'</td>\n",
       "      <td>'wFre'</td>\n",
       "      <td>2</td>\n",
       "      <td>0.927235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>'sxu'</td>\n",
       "      <td>'Cw2vBw2vLstmCrf'</td>\n",
       "      <td>'wFre'</td>\n",
       "      <td>3</td>\n",
       "      <td>0.990005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>'sxu'</td>\n",
       "      <td>'Cw2vBw2vLstmCrf'</td>\n",
       "      <td>'sLen'</td>\n",
       "      <td>0</td>\n",
       "      <td>0.944223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>'sxu'</td>\n",
       "      <td>'Cw2vBw2vLstmCrf'</td>\n",
       "      <td>'sLen'</td>\n",
       "      <td>1</td>\n",
       "      <td>0.915481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>'sxu'</td>\n",
       "      <td>'Cw2vBw2vLstmCrf'</td>\n",
       "      <td>'sLen'</td>\n",
       "      <td>2</td>\n",
       "      <td>0.931108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>'sxu'</td>\n",
       "      <td>'Cw2vBw2vLstmCrf'</td>\n",
       "      <td>'sLen'</td>\n",
       "      <td>3</td>\n",
       "      <td>0.946928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>'sxu'</td>\n",
       "      <td>'Cw2vBw2vLstmCrf'</td>\n",
       "      <td>'wLen'</td>\n",
       "      <td>0</td>\n",
       "      <td>0.950465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>'sxu'</td>\n",
       "      <td>'Cw2vBw2vLstmCrf'</td>\n",
       "      <td>'wLen'</td>\n",
       "      <td>1</td>\n",
       "      <td>0.963395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>'sxu'</td>\n",
       "      <td>'Cw2vBw2vLstmCrf'</td>\n",
       "      <td>'wLen'</td>\n",
       "      <td>2</td>\n",
       "      <td>0.873945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>'sxu'</td>\n",
       "      <td>'Cw2vBw2vLstmCrf'</td>\n",
       "      <td>'wLen'</td>\n",
       "      <td>3</td>\n",
       "      <td>0.136609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>'sxu'</td>\n",
       "      <td>'Cw2vBw2vLstmCrf'</td>\n",
       "      <td>'wCon'</td>\n",
       "      <td>0</td>\n",
       "      <td>0.735192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>'sxu'</td>\n",
       "      <td>'Cw2vBw2vLstmCrf'</td>\n",
       "      <td>'wCon'</td>\n",
       "      <td>1</td>\n",
       "      <td>0.853792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>'sxu'</td>\n",
       "      <td>'Cw2vBw2vLstmCrf'</td>\n",
       "      <td>'wCon'</td>\n",
       "      <td>2</td>\n",
       "      <td>0.984895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>'sxu'</td>\n",
       "      <td>'Cw2vBw2vLstmCrf'</td>\n",
       "      <td>'wCon'</td>\n",
       "      <td>3</td>\n",
       "      <td>0.989439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>'sxu'</td>\n",
       "      <td>'Cw2vBw2vLstmCrf'</td>\n",
       "      <td>'tag'</td>\n",
       "      <td>0</td>\n",
       "      <td>0.950465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>'sxu'</td>\n",
       "      <td>'Cw2vBw2vLstmCrf'</td>\n",
       "      <td>'tag'</td>\n",
       "      <td>1</td>\n",
       "      <td>0.963395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>'sxu'</td>\n",
       "      <td>'Cw2vBw2vLstmCrf'</td>\n",
       "      <td>'tag'</td>\n",
       "      <td>2</td>\n",
       "      <td>0.873945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>'sxu'</td>\n",
       "      <td>'Cw2vBw2vLstmCrf'</td>\n",
       "      <td>'tag'</td>\n",
       "      <td>3</td>\n",
       "      <td>0.718553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>'sxu'</td>\n",
       "      <td>'Cw2vBw2vLstmCrf'</td>\n",
       "      <td>'cCon'</td>\n",
       "      <td>0</td>\n",
       "      <td>0.940989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>'sxu'</td>\n",
       "      <td>'Cw2vBw2vLstmCrf'</td>\n",
       "      <td>'cCon'</td>\n",
       "      <td>1</td>\n",
       "      <td>0.847237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250</th>\n",
       "      <td>'ncc'</td>\n",
       "      <td>'CelmBnonLstmMlp'</td>\n",
       "      <td>'oDen'</td>\n",
       "      <td>2</td>\n",
       "      <td>0.918204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1251</th>\n",
       "      <td>'ncc'</td>\n",
       "      <td>'CelmBnonLstmMlp'</td>\n",
       "      <td>'oDen'</td>\n",
       "      <td>3</td>\n",
       "      <td>0.898406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1252</th>\n",
       "      <td>'ncc'</td>\n",
       "      <td>'CelmBnonLstmMlp'</td>\n",
       "      <td>'cFre'</td>\n",
       "      <td>0</td>\n",
       "      <td>0.818964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>'ncc'</td>\n",
       "      <td>'CelmBnonLstmMlp'</td>\n",
       "      <td>'cFre'</td>\n",
       "      <td>1</td>\n",
       "      <td>0.843948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>'ncc'</td>\n",
       "      <td>'CelmBnonLstmMlp'</td>\n",
       "      <td>'cFre'</td>\n",
       "      <td>2</td>\n",
       "      <td>0.899153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>'ncc'</td>\n",
       "      <td>'CelmBnonLstmMlp'</td>\n",
       "      <td>'cFre'</td>\n",
       "      <td>3</td>\n",
       "      <td>0.956644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>'ncc'</td>\n",
       "      <td>'CelmBnonLstmMlp'</td>\n",
       "      <td>'wFre'</td>\n",
       "      <td>0</td>\n",
       "      <td>0.825545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>'ncc'</td>\n",
       "      <td>'CelmBnonLstmMlp'</td>\n",
       "      <td>'wFre'</td>\n",
       "      <td>1</td>\n",
       "      <td>0.758793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258</th>\n",
       "      <td>'ncc'</td>\n",
       "      <td>'CelmBnonLstmMlp'</td>\n",
       "      <td>'wFre'</td>\n",
       "      <td>2</td>\n",
       "      <td>0.900714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>'ncc'</td>\n",
       "      <td>'CelmBnonLstmMlp'</td>\n",
       "      <td>'wFre'</td>\n",
       "      <td>3</td>\n",
       "      <td>0.984893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1260</th>\n",
       "      <td>'ncc'</td>\n",
       "      <td>'CelmBnonLstmMlp'</td>\n",
       "      <td>'sLen'</td>\n",
       "      <td>0</td>\n",
       "      <td>0.923457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1261</th>\n",
       "      <td>'ncc'</td>\n",
       "      <td>'CelmBnonLstmMlp'</td>\n",
       "      <td>'sLen'</td>\n",
       "      <td>1</td>\n",
       "      <td>0.905501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1262</th>\n",
       "      <td>'ncc'</td>\n",
       "      <td>'CelmBnonLstmMlp'</td>\n",
       "      <td>'sLen'</td>\n",
       "      <td>2</td>\n",
       "      <td>0.902090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1263</th>\n",
       "      <td>'ncc'</td>\n",
       "      <td>'CelmBnonLstmMlp'</td>\n",
       "      <td>'sLen'</td>\n",
       "      <td>3</td>\n",
       "      <td>0.902540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1264</th>\n",
       "      <td>'ncc'</td>\n",
       "      <td>'CelmBnonLstmMlp'</td>\n",
       "      <td>'wLen'</td>\n",
       "      <td>0</td>\n",
       "      <td>0.935621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1265</th>\n",
       "      <td>'ncc'</td>\n",
       "      <td>'CelmBnonLstmMlp'</td>\n",
       "      <td>'wLen'</td>\n",
       "      <td>1</td>\n",
       "      <td>0.940352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266</th>\n",
       "      <td>'ncc'</td>\n",
       "      <td>'CelmBnonLstmMlp'</td>\n",
       "      <td>'wLen'</td>\n",
       "      <td>2</td>\n",
       "      <td>0.785363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267</th>\n",
       "      <td>'ncc'</td>\n",
       "      <td>'CelmBnonLstmMlp'</td>\n",
       "      <td>'wLen'</td>\n",
       "      <td>3</td>\n",
       "      <td>0.277080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1268</th>\n",
       "      <td>'ncc'</td>\n",
       "      <td>'CelmBnonLstmMlp'</td>\n",
       "      <td>'wCon'</td>\n",
       "      <td>0</td>\n",
       "      <td>0.671775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1269</th>\n",
       "      <td>'ncc'</td>\n",
       "      <td>'CelmBnonLstmMlp'</td>\n",
       "      <td>'wCon'</td>\n",
       "      <td>1</td>\n",
       "      <td>0.839372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1270</th>\n",
       "      <td>'ncc'</td>\n",
       "      <td>'CelmBnonLstmMlp'</td>\n",
       "      <td>'wCon'</td>\n",
       "      <td>2</td>\n",
       "      <td>0.974156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1271</th>\n",
       "      <td>'ncc'</td>\n",
       "      <td>'CelmBnonLstmMlp'</td>\n",
       "      <td>'wCon'</td>\n",
       "      <td>3</td>\n",
       "      <td>0.976574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1272</th>\n",
       "      <td>'ncc'</td>\n",
       "      <td>'CelmBnonLstmMlp'</td>\n",
       "      <td>'tag'</td>\n",
       "      <td>0</td>\n",
       "      <td>0.941031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1273</th>\n",
       "      <td>'ncc'</td>\n",
       "      <td>'CelmBnonLstmMlp'</td>\n",
       "      <td>'tag'</td>\n",
       "      <td>1</td>\n",
       "      <td>0.948687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>'ncc'</td>\n",
       "      <td>'CelmBnonLstmMlp'</td>\n",
       "      <td>'tag'</td>\n",
       "      <td>2</td>\n",
       "      <td>0.825604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275</th>\n",
       "      <td>'ncc'</td>\n",
       "      <td>'CelmBnonLstmMlp'</td>\n",
       "      <td>'tag'</td>\n",
       "      <td>3</td>\n",
       "      <td>0.724470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1276</th>\n",
       "      <td>'ncc'</td>\n",
       "      <td>'CelmBnonLstmMlp'</td>\n",
       "      <td>'cCon'</td>\n",
       "      <td>0</td>\n",
       "      <td>0.901772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277</th>\n",
       "      <td>'ncc'</td>\n",
       "      <td>'CelmBnonLstmMlp'</td>\n",
       "      <td>'cCon'</td>\n",
       "      <td>1</td>\n",
       "      <td>0.841252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278</th>\n",
       "      <td>'ncc'</td>\n",
       "      <td>'CelmBnonLstmMlp'</td>\n",
       "      <td>'cCon'</td>\n",
       "      <td>2</td>\n",
       "      <td>0.979774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1279</th>\n",
       "      <td>'ncc'</td>\n",
       "      <td>'CelmBnonLstmMlp'</td>\n",
       "      <td>'cCon'</td>\n",
       "      <td>3</td>\n",
       "      <td>0.995035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1280 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     dataset              model attribute bucket        f1\n",
       "0      'sxu'  'Cw2vBw2vLstmCrf'    'oDen'      0  0.955938\n",
       "1      'sxu'  'Cw2vBw2vLstmCrf'    'oDen'      1  0.941879\n",
       "2      'sxu'  'Cw2vBw2vLstmCrf'    'oDen'      2  0.929166\n",
       "3      'sxu'  'Cw2vBw2vLstmCrf'    'oDen'      3  0.934691\n",
       "4      'sxu'  'Cw2vBw2vLstmCrf'    'cFre'      0  0.876042\n",
       "5      'sxu'  'Cw2vBw2vLstmCrf'    'cFre'      1  0.891689\n",
       "6      'sxu'  'Cw2vBw2vLstmCrf'    'cFre'      2  0.940681\n",
       "7      'sxu'  'Cw2vBw2vLstmCrf'    'cFre'      3  0.972888\n",
       "8      'sxu'  'Cw2vBw2vLstmCrf'    'wFre'      0  0.849907\n",
       "9      'sxu'  'Cw2vBw2vLstmCrf'    'wFre'      1  0.814121\n",
       "10     'sxu'  'Cw2vBw2vLstmCrf'    'wFre'      2  0.927235\n",
       "11     'sxu'  'Cw2vBw2vLstmCrf'    'wFre'      3  0.990005\n",
       "12     'sxu'  'Cw2vBw2vLstmCrf'    'sLen'      0  0.944223\n",
       "13     'sxu'  'Cw2vBw2vLstmCrf'    'sLen'      1  0.915481\n",
       "14     'sxu'  'Cw2vBw2vLstmCrf'    'sLen'      2  0.931108\n",
       "15     'sxu'  'Cw2vBw2vLstmCrf'    'sLen'      3  0.946928\n",
       "16     'sxu'  'Cw2vBw2vLstmCrf'    'wLen'      0  0.950465\n",
       "17     'sxu'  'Cw2vBw2vLstmCrf'    'wLen'      1  0.963395\n",
       "18     'sxu'  'Cw2vBw2vLstmCrf'    'wLen'      2  0.873945\n",
       "19     'sxu'  'Cw2vBw2vLstmCrf'    'wLen'      3  0.136609\n",
       "20     'sxu'  'Cw2vBw2vLstmCrf'    'wCon'      0  0.735192\n",
       "21     'sxu'  'Cw2vBw2vLstmCrf'    'wCon'      1  0.853792\n",
       "22     'sxu'  'Cw2vBw2vLstmCrf'    'wCon'      2  0.984895\n",
       "23     'sxu'  'Cw2vBw2vLstmCrf'    'wCon'      3  0.989439\n",
       "24     'sxu'  'Cw2vBw2vLstmCrf'     'tag'      0  0.950465\n",
       "25     'sxu'  'Cw2vBw2vLstmCrf'     'tag'      1  0.963395\n",
       "26     'sxu'  'Cw2vBw2vLstmCrf'     'tag'      2  0.873945\n",
       "27     'sxu'  'Cw2vBw2vLstmCrf'     'tag'      3  0.718553\n",
       "28     'sxu'  'Cw2vBw2vLstmCrf'    'cCon'      0  0.940989\n",
       "29     'sxu'  'Cw2vBw2vLstmCrf'    'cCon'      1  0.847237\n",
       "...      ...                ...       ...    ...       ...\n",
       "1250   'ncc'  'CelmBnonLstmMlp'    'oDen'      2  0.918204\n",
       "1251   'ncc'  'CelmBnonLstmMlp'    'oDen'      3  0.898406\n",
       "1252   'ncc'  'CelmBnonLstmMlp'    'cFre'      0  0.818964\n",
       "1253   'ncc'  'CelmBnonLstmMlp'    'cFre'      1  0.843948\n",
       "1254   'ncc'  'CelmBnonLstmMlp'    'cFre'      2  0.899153\n",
       "1255   'ncc'  'CelmBnonLstmMlp'    'cFre'      3  0.956644\n",
       "1256   'ncc'  'CelmBnonLstmMlp'    'wFre'      0  0.825545\n",
       "1257   'ncc'  'CelmBnonLstmMlp'    'wFre'      1  0.758793\n",
       "1258   'ncc'  'CelmBnonLstmMlp'    'wFre'      2  0.900714\n",
       "1259   'ncc'  'CelmBnonLstmMlp'    'wFre'      3  0.984893\n",
       "1260   'ncc'  'CelmBnonLstmMlp'    'sLen'      0  0.923457\n",
       "1261   'ncc'  'CelmBnonLstmMlp'    'sLen'      1  0.905501\n",
       "1262   'ncc'  'CelmBnonLstmMlp'    'sLen'      2  0.902090\n",
       "1263   'ncc'  'CelmBnonLstmMlp'    'sLen'      3  0.902540\n",
       "1264   'ncc'  'CelmBnonLstmMlp'    'wLen'      0  0.935621\n",
       "1265   'ncc'  'CelmBnonLstmMlp'    'wLen'      1  0.940352\n",
       "1266   'ncc'  'CelmBnonLstmMlp'    'wLen'      2  0.785363\n",
       "1267   'ncc'  'CelmBnonLstmMlp'    'wLen'      3  0.277080\n",
       "1268   'ncc'  'CelmBnonLstmMlp'    'wCon'      0  0.671775\n",
       "1269   'ncc'  'CelmBnonLstmMlp'    'wCon'      1  0.839372\n",
       "1270   'ncc'  'CelmBnonLstmMlp'    'wCon'      2  0.974156\n",
       "1271   'ncc'  'CelmBnonLstmMlp'    'wCon'      3  0.976574\n",
       "1272   'ncc'  'CelmBnonLstmMlp'     'tag'      0  0.941031\n",
       "1273   'ncc'  'CelmBnonLstmMlp'     'tag'      1  0.948687\n",
       "1274   'ncc'  'CelmBnonLstmMlp'     'tag'      2  0.825604\n",
       "1275   'ncc'  'CelmBnonLstmMlp'     'tag'      3  0.724470\n",
       "1276   'ncc'  'CelmBnonLstmMlp'    'cCon'      0  0.901772\n",
       "1277   'ncc'  'CelmBnonLstmMlp'    'cCon'      1  0.841252\n",
       "1278   'ncc'  'CelmBnonLstmMlp'    'cCon'      2  0.979774\n",
       "1279   'ncc'  'CelmBnonLstmMlp'    'cCon'      3  0.995035\n",
       "\n",
       "[1280 rows x 5 columns]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cws_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 5\n"
     ]
    }
   ],
   "source": [
    "#ns_gp_result_f1\n",
    "pca_result_cws, cp_result_cws, xgb_result_cws, gp_result_cws,lg_result_cws, bs_result_cws = compare_models(cws_df, cws_t, [], \\\n",
    "                                                                     cws_index, 5, data='cws',\\\n",
    "                                                                     scale_back=False,\\\n",
    "                                                                     reg_E=1, reg_J=1\\\n",
    "                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.022234488, 0.025269667, 0.030292174, 0.040998533, 0.026698781]"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_result_cws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.034425203, 0.047388755, 0.04387816, 0.046927255, 0.041738246]"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp_result_cws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.02238433635627803,\n",
       " 0.016810709704798858,\n",
       " 0.01885663775968977,\n",
       " 0.027082089256251975,\n",
       " 0.021411307921565745]"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_result_cws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0892792442340237,\n",
       " 0.07384662082151357,\n",
       " 0.08242185138624998,\n",
       " 0.10057866934186983,\n",
       " 0.10194449660125048]"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp_result_cws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.03486975941658747,\n",
       " 0.03472427003199159,\n",
       " 0.04159289545887568,\n",
       " 0.050123173220666596,\n",
       " 0.04158469596137467]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg_result_cws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.12049468931846594,\n",
       " 0.13820834763262552,\n",
       " 0.14978932801740874,\n",
       " 0.16011736309027552,\n",
       " 0.11410698791924323]"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs_result_cws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.029098729"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(pca_result_cws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.042871527"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cp_result_cws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.021309016199716875"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(xgb_result_cws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08961417647698151"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(gp_result_cws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.040578958817899205"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(lg_result_cws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13654334319560377"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(bs_result_cws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
