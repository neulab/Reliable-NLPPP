{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "### Helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import copy\n",
    "import random\n",
    "import pickle\n",
    "import itertools\n",
    "import math\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.transforms as mtransforms\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "\n",
    "import tensorly as tl\n",
    "from tensorly.decomposition import parafac\n",
    "from tensorly.decomposition import tucker\n",
    "from tensorly import tucker_to_tensor\n",
    "from tensorly.decomposition import robust_pca\n",
    "\n",
    "import lightgbm as lgb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "* Functions about tensor operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# FOR F1 TENSOR\n",
    "def get_dict_f1(path, metric=\"f1\"):\n",
    "    with open(path, \"rb\") as f:\n",
    "        tensorDict_list = pickle.load(f)\n",
    "        if metric == \"f1\":\n",
    "            d = tensorDict_list[0]\n",
    "    return d\n",
    "     \n",
    "\n",
    "def iter_dict_f1(tensor_dict, mode_dict, dept=0):\n",
    "    for k,v in tensor_dict.items():\n",
    "        if isinstance(v,dict):\n",
    "            mode_dict[dept].add(k)\n",
    "            iter_dict_f1(v, mode_dict, dept+1)\n",
    "        else:\n",
    "            mode_dict[dept].add(k)\n",
    "            mode_dict[dept+1].add(v)\n",
    "            \n",
    "    \n",
    "def get_modes_f1(tensorDict, numMode):\n",
    "    # numMode + 1 since values take 1 extra level \n",
    "    modeDict = {i: set() for i in range(numMode+1)} \n",
    "    iter_dict_f1(tensorDict, modeDict)\n",
    "    return modeDict\n",
    "    \n",
    "def order_buckets_f1(d):\n",
    "    d_copy = copy.deepcopy(d)\n",
    "    for model in d.keys():\n",
    "        for dataset in d[model].keys():\n",
    "            for attribute in d[model][dataset].keys():\n",
    "                d_copy[model][dataset][attribute] = {}\n",
    "                for i, (k, v) in enumerate(d[model][dataset][attribute].items()):\n",
    "                    d_copy[model][dataset][attribute][i] = v\n",
    "    return d_copy\n",
    "\n",
    "# Directions: model -> dataset -> attribute -> bucket\n",
    "def get_index_f1(mode_dict):\n",
    "    index_lst = []\n",
    "    for m in mode_dict:\n",
    "        md = dict()\n",
    "        for i, n in enumerate(list(mode_dict[m])):\n",
    "            md[n] = i\n",
    "        index_lst.append(md)\n",
    "    return index_lst\n",
    "\n",
    "\n",
    "def read_dict_f1(dataDict, maplist):\n",
    "    first, rest = maplist[0], maplist[1:]\n",
    "    # Handle missing data\n",
    "    if dataDict.get(first) == None:\n",
    "        return None\n",
    "    if rest: \n",
    "        return read_dict_f1(dataDict[first], rest)\n",
    "    else:\n",
    "        # return None if key is missing\n",
    "        return dataDict.get(first) \n",
    "\n",
    "    \n",
    "def read_tensor_f1(tensor, maplist):\n",
    "    first, rest = maplist[0], maplist[1:]\n",
    "    if rest: \n",
    "        return read_tensor_f1(tensor[first], rest)\n",
    "    else:\n",
    "        return tensor[first]\n",
    "\n",
    "\n",
    "def write_tensor_f1(tensor, maplist, val):\n",
    "    first, rest = maplist[0], maplist[1:]\n",
    "\n",
    "    if rest: \n",
    "        assert(isinstance(tensor[first], np.ndarray))\n",
    "        return write_tensor_f1(tensor[first], rest, val)\n",
    "    else:\n",
    "        if isinstance(tensor[first], np.ndarray):\n",
    "            print(\"Maplist length not correct!\")\n",
    "        tensor[first] = val\n",
    "    \n",
    "    \n",
    "# Construct a tensor of dimensions *argv    \n",
    "def init_tensor_f1(*argv):\n",
    "    size = 1\n",
    "    for s in argv:\n",
    "        size *= s\n",
    "    X = tl.tensor(np.arange(size).reshape(argv), dtype=tl.float32)\n",
    "    return X\n",
    "\n",
    "\n",
    "def convert_index_f1(p, indDict):\n",
    "    l = []\n",
    "    for i,n in enumerate(p):\n",
    "        l.append(indDict[i][n])\n",
    "    return l\n",
    "\n",
    "    \n",
    "def construct_tensor_f1(modeDict, tensorDict, indexDict):\n",
    "    l = []\n",
    "    for _,v in modeDict.items():\n",
    "        l.append(list(v))\n",
    "    from_paths = list(itertools.product(*l[:-1])) # all possible entries \n",
    "    \n",
    "    to_paths = []\n",
    "    for p in from_paths:\n",
    "        to_paths.append(convert_index(p, indexDict))\n",
    "  \n",
    "    \n",
    "    # tensor dim\n",
    "    dims = [len(modeDict[i]) for i in modeDict.keys()][:-1]\n",
    "    X = init_tensor_f1(*dims)\n",
    "    \n",
    "    \n",
    "    for p in range(len(from_paths)):\n",
    "       # print(p)\n",
    "        val = read_dict_f1(tensorDict, from_paths[p])\n",
    "        # if data is not missing\n",
    "        if val != None:\n",
    "            write_tensor_f1(X,to_paths[p],val) \n",
    "        else:\n",
    "            print(\"MISSING DATA AT \"+ str(from_paths[p]))\n",
    "            write_tensor_f1(X,to_paths[p],None) \n",
    "    return X\n",
    "        \n",
    "def all_paths_f1(modeDict):\n",
    "    l = []\n",
    "    for _,v in modeDict.items():\n",
    "        l.append(list(v))\n",
    "    p = list(itertools.product(*l[:-1])) # all possible entries \n",
    "    return p\n",
    "\n",
    "\n",
    "def convert_to_df_f1(tensor, mode, index):\n",
    "    all_p = all_paths_f1(mode)\n",
    "    name = [\"features\"]\n",
    "    df = pd.DataFrame(index=range(len(all_p)),columns=name)\n",
    "    f1_list = []\n",
    "    for i in range(len(all_p)):\n",
    "        paste = str(all_p[i])[1:-1]\n",
    "        df.iloc[i,:]['features'] = paste\n",
    "        ind = convert_index_f1(all_p[i], index)\n",
    "        f1 = read_tensor_f1(tensor, ind)\n",
    "        f1_list.append(f1)\n",
    "    \n",
    "    df[['model','dataset',\"attribute\",\"bucket\"]] = df['features'].str.split(', ',expand=True)\n",
    "    df = df.drop(['features'], axis=1)\n",
    "    df['f1'] = f1_list\n",
    "    return df\n",
    "\n",
    "def categorize_df_f1(df, ind_lst):\n",
    "    df_new = copy.deepcopy(df)\n",
    "    for i in range(len(df.columns)-2):\n",
    "        col_dict = ind_lst[i]\n",
    "        for j in range(len(df.index)):\n",
    "            entry = df.iloc[j,i]\n",
    "            entry = str(entry)[1:len(entry)-1]\n",
    "            val = col_dict[str(entry)]\n",
    "            df_new.iloc[j,i] = val\n",
    "    df_new['bucket'] = pd.to_numeric(df_new['bucket'])\n",
    "    return df_new\n",
    "        \n",
    "\n",
    "def scale_f1(tensor, mode_dict, index_dict):\n",
    "    all_p = all_paths_f1(mode_dict)\n",
    "    paths = convert_paths(all_p,index_dict)\n",
    "    tensor2 = copy.deepcopy(tensor)\n",
    "    array = tensor.flatten()\n",
    "    m, sd = np.mean(array), np.std(array)\n",
    "    for i in paths:\n",
    "        val = read_tensor_f1(tensor, i)\n",
    "        transformed = (val - m) / sd\n",
    "        write_tensor_f1(tensor2, i, transformed) \n",
    "    return tensor2, m, sd\n",
    "\n",
    "def scale_back_f1(val, mean, sd):\n",
    "    scale_back = val * sd + mean\n",
    "    return scale_back "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "def get_modes(df, col_indices):\n",
    "    numModes = len(col_indices)\n",
    "    modeDict = {i: None for i in range(numModes)} \n",
    "    colInd = {j: None for j in col_indices} \n",
    "    for i in modeDict:\n",
    "        c = col_indices[i]\n",
    "        col = df.iloc[:,c]\n",
    "        vals = pd.unique(col)\n",
    "        modeDict[i] = set(vals)\n",
    "        colInd[c] = i\n",
    "    return modeDict, colInd\n",
    "\n",
    "    \n",
    "    modeDict = {i: None for i in col_indices} \n",
    "    for i in col_indices:\n",
    "        col = df.iloc[:,i]\n",
    "        vals = pd.unique(col)\n",
    "        modeDict[i] = set(vals)\n",
    "    return modeDict\n",
    "\n",
    "def get_index(mode_dict):\n",
    "    index_lst = []\n",
    "    for m in mode_dict:\n",
    "        md = dict()\n",
    "        for i, n in enumerate(list(mode_dict[m])):\n",
    "            md[n] = i\n",
    "        index_lst.append(md)\n",
    "    return index_lst\n",
    "\n",
    "\n",
    "def all_paths(mode_dict):\n",
    "    l = []\n",
    "    for _,v in mode_dict.items():\n",
    "        l.append(list(v))\n",
    "    p = list(itertools.product(*l[:])) # all possible entries \n",
    "    return p\n",
    "\n",
    "\n",
    "def init_tensor(modeDict):\n",
    "    sl = []\n",
    "    prod = 1\n",
    "    for f in modeDict:\n",
    "        size = len(modeDict[f])\n",
    "        sl.append(size)\n",
    "        prod *= size\n",
    "    t = tl.zeros(sl)\n",
    "    return t   \n",
    "\n",
    "def read_tensor(tensor, maplist):\n",
    "    first, rest = maplist[0], maplist[1:]\n",
    "    if rest: \n",
    "        return read_tensor(tensor[first], rest)\n",
    "    else:\n",
    "        return tensor[first]\n",
    "\n",
    "def scale(df, cols):\n",
    "    new = df.copy()\n",
    "    for i in cols:\n",
    "        new.iloc[:,i] = preprocessing.scale(df.iloc[:,i])\n",
    "    return new\n",
    "\n",
    "def path_to_row(df, source='Source', target='Target'):\n",
    "    path_dict = {}\n",
    "    for i in range(len(df)):\n",
    "        row = df.iloc[i,:]\n",
    "        s, t = row[source], row[target]\n",
    "        path_dict[(s,t)] = i\n",
    "    return path_dict\n",
    "        \n",
    "def get_val(df, path, rows):\n",
    "    s, t, b = path[0], path[1], path[2]\n",
    "    if (s,t) in rows:\n",
    "        r = rows[(s,t)]\n",
    "        val = df.loc[r,b]\n",
    "        return val\n",
    "    else: return None\n",
    "    \n",
    "def convert_index(p, indDict):\n",
    "    l = []\n",
    "    for i,n in enumerate(p):\n",
    "        l.append(indDict[i][n])\n",
    "    return l\n",
    "\n",
    "def write_tensor(tensor, maplist, val):\n",
    "    first, rest = maplist[0], maplist[1:]\n",
    "\n",
    "    if rest: \n",
    "        assert(isinstance(tensor[first], np.ndarray))\n",
    "        return write_tensor(tensor[first], rest, val)\n",
    "    else:\n",
    "        if isinstance(tensor[first], np.ndarray):\n",
    "            print(\"Maplist length not correct!\")\n",
    "        tensor[first] = val\n",
    "    \n",
    "def convert_paths(from_names, indexDict):\n",
    "    to_paths = []\n",
    "    for p in from_names:\n",
    "        to_paths.append(convert_index(p, indexDict))\n",
    "    return to_paths\n",
    "\n",
    "    \n",
    "def fill_tensor(tensor, df, path_lst, index_lst, source='Source', target='Target'):\n",
    "    row_language = path_to_row(df, source=source, target=target)\n",
    "    missing = []\n",
    "    for p in path_lst:\n",
    "        val = get_val(df, p, row_language)\n",
    "        index = convert_index(p, index_lst)\n",
    "        # If the path exists and value is not missing \n",
    "        if val != None and not math.isnan(val):\n",
    "            write_tensor(tensor, index, val)\n",
    "        else:\n",
    "            write_tensor(tensor, index, 0)\n",
    "            missing.append(p)\n",
    "    return tensor, missing\n",
    "\n",
    "                    \n",
    "def create_mask(missing, tensor):\n",
    "    shape = tensor.shape\n",
    "    mask = tl.ones(shape, dtype=tl.float32)\n",
    "    for r in missing:\n",
    "        write_tensor(mask, r,0)\n",
    "    return mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "def convert_to_df_cws(tensor, mode, index):\n",
    "    all_p = all_paths_f1(mode)\n",
    "    name = [\"features\"]\n",
    "    df = pd.DataFrame(index=range(len(all_p)),columns=name)\n",
    "    f1_list = []\n",
    "    for i in range(len(all_p)):\n",
    "        paste = str(all_p[i])[1:-1]\n",
    "        df.iloc[i,:]['features'] = paste\n",
    "        \n",
    "        \n",
    "        ind = convert_index_f1(all_p[i], index)\n",
    "        f1 = read_tensor_f1(tensor, ind)\n",
    "        f1_list.append(f1)\n",
    "    \n",
    "    df[['dataset','model',\"attribute\",\"bucket\"]] = df['features'].str.split(', ',expand=True)\n",
    "    df = df.drop(['features'], axis=1)\n",
    "    df['f1'] = f1_list\n",
    "    return df\n",
    "\n",
    "def categorize_df_cws(df, ind_lst):\n",
    "    df_new = copy.deepcopy(df)\n",
    "    for i in range(len(df.columns)-2):\n",
    "        col_dict = ind_lst[i]\n",
    "        for j in range(len(df.index)):\n",
    "            entry = df.iloc[j,i]\n",
    "            entry = str(entry)[1:len(entry)-1]\n",
    "            val = col_dict[str(entry)]\n",
    "            df_new.iloc[j,i] = val\n",
    "    df_new['bucket'] = pd.to_numeric(df_new['bucket'])\n",
    "    return df_new\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "def reconstruct(tensor, mask_tensor, decomp ='pca'):\n",
    "\n",
    "    if decomp == 'pca':\n",
    "        masked_tensor = tensor * mask_tensor\n",
    "        pca_res = robust_pca(masked_tensor, mask=mask_tensor)\n",
    "        reconstructed = pca_res[0]\n",
    "        \n",
    "    if decomp == 'tucker_decomp': \n",
    "        masked_tensor = tensor * mask_tensor\n",
    "        core, factors = tucker(masked_tensor, rank = [39,44,22])\n",
    "        reconstructed = tucker_to_tensor((core, factors))\n",
    "\n",
    "    if decomp == 'cp':\n",
    "        masked_tensor = tensor * mask_tensor\n",
    "        (w, f), err = parafac(masked_tensor, rank=5, mask = mask_tensor, return_errors = True)\n",
    "        reconstructed = tl.kruskal_to_tensor((w, f))\n",
    "    \n",
    "    return reconstructed\n",
    "\n",
    "def read_val(ind_list, tensor, s, t, f):\n",
    "    s, t, f = ind_list[0][s], ind_list[1][t], ind_list[2][f]\n",
    "    return tensor[s][t][f]\n",
    "\n",
    "def transform(orig_col, val):\n",
    "    m, sd = orig_col.mean(), orig_col.std()\n",
    "    orig = val * sd + m\n",
    "    return orig\n",
    "\n",
    "# Construct order 3 tensor\n",
    "def construct_tensor_from_df(df, lang_modes=[0,1], need_scale=True,\\\n",
    "                            source='Source', target='Target', feat_mode=None): \n",
    "    # Get first 2 lang modes\n",
    "    modeDict, colInd = get_modes(df, lang_modes)\n",
    "    \n",
    "    # Specify axis 3\n",
    "    buckets = df.columns[feat_mode]\n",
    "    modeDict[2] = set(buckets)\n",
    "    \n",
    "    # Get all possible indices for the tensor\n",
    "    all_p = all_paths(modeDict)\n",
    "    indLst = get_index(modeDict)\n",
    "    \n",
    "    # Init empty tensors\n",
    "    t = init_tensor(modeDict)\n",
    "    \n",
    "    if need_scale:\n",
    "        # Scaled version\n",
    "        df2 = scale(df, [feat_mode])\n",
    "    else:\n",
    "        df2 = copy.deepcopy(df) \n",
    "        \n",
    "    # Create a tensor\n",
    "    t_bleu, missing = fill_tensor(t, df2, all_p, indLst, source=source, target=target)\n",
    "    return t_bleu, missing, modeDict, indLst, all_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Functions to perform k-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "def train_test_split(df, fold=5):\n",
    "    rows = list(range(len(df)))\n",
    "    kf = KFold(n_splits=fold)\n",
    "    \n",
    "    test_lst = []\n",
    "    train_lst = []\n",
    "    for train, test in kf.split(rows):\n",
    "        train_lst.append(train)\n",
    "        test_lst.append(test)\n",
    "        \n",
    "    shuffled_df = df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    folds = []\n",
    "    for i in range(len(test_lst)):\n",
    "        index_train = list(train_lst[i])\n",
    "        index_test = list(test_lst[i])\n",
    "        train = shuffled_df.iloc[index_train,:]\n",
    "        test = shuffled_df.iloc[index_test,:]\n",
    "        folds.append((train, test))\n",
    "    return folds, shuffled_df\n",
    "\n",
    "\n",
    "def mask_out_test_y(test_y_df, missing, tensor, index_dict, data):\n",
    "    new_missing = copy.deepcopy(missing)\n",
    "    new_tensor = copy.deepcopy(tensor)\n",
    "    test_entries = []\n",
    "    # Mask out BLEUs for test entries\n",
    "    for index, row in test_y_df.iterrows():\n",
    "        if data == 'wiki':\n",
    "            source, target, bleu = row['Source'], row['Target'], 'BLEU'\n",
    "            \n",
    "        if data == 'tsf':\n",
    "            source, target, bleu = row['Task lang'], row['Aux lang'], 'Accuracy'\n",
    "            \n",
    "        if data == 'tsfmt':\n",
    "            source, target, bleu = row[' Source lang'], row['Transfer lang'], 'BLEU'\n",
    "            \n",
    "        path = (source, target, bleu)\n",
    "        new_missing.append(path)\n",
    "        test_entries.append(path)\n",
    "        \n",
    "    missing_combined = convert_paths(new_missing, index_dict)\n",
    "    new_mask = create_mask(missing_combined, tensor)\n",
    "    return new_mask, test_entries\n",
    "\n",
    "\n",
    "def mask_out_test_y_f1(test_y_df, missing, tensor, index_dict):\n",
    "    new_missing = copy.deepcopy(missing)\n",
    "    new_tensor = copy.deepcopy(tensor)\n",
    "    test_entries = []\n",
    "    # Mask out f1 for test entries\n",
    "    for index, row in test_y_df.iterrows():\n",
    "        mod, dat, attr, buck = row['model'][1:-1], row['dataset'][1:-1], row['attribute'][1:-1], row['bucket']\n",
    "        path = (mod, dat, attr, int(buck))\n",
    "        #print(path)\n",
    "        new_missing.append(path)\n",
    "        test_entries.append(path)\n",
    "        \n",
    "    missing_combined = convert_paths(new_missing, index_dict)\n",
    "    new_mask = create_mask(missing_combined, tensor)\n",
    "    return new_mask, test_entries\n",
    "\n",
    "def mask_out_test_y_cws(test_y_df, missing, tensor, index_dict):\n",
    "    new_missing = copy.deepcopy(missing)\n",
    "    new_tensor = copy.deepcopy(tensor)\n",
    "    test_entries = []\n",
    "    # Mask out f1 for test entries\n",
    "    for index, row in test_y_df.iterrows():\n",
    "        dat, mod, attr, buck = row['dataset'][1:-1], row['model'][1:-1], row['attribute'][1:-1], row['bucket']\n",
    "        path = (dat, mod, attr, int(buck))\n",
    "        #print(path)\n",
    "        new_missing.append(path)\n",
    "        test_entries.append(path)\n",
    "        \n",
    "    missing_combined = convert_paths(new_missing, index_dict)\n",
    "    new_mask = create_mask(missing_combined, tensor)\n",
    "    return new_mask, test_entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing performance prediction models for different NLP tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Baseline, XGBoost, LGBM, CP and PCA performance prediction \n",
    "# models on the same test sets using k-fold cross-validation \n",
    "# Returns a list of RMSE for each fold for each model \n",
    "\n",
    "def compare_models(df, tensor, missing, index_lst,\\\n",
    "                   fold, scale_back=True, \\\n",
    "                   cp_rank=3, data=\"wiki\", \\\n",
    "                   non_scaled_tensor=None,\\\n",
    "                  non_scaled_mode_dict=None,\\\n",
    "                  non_scaled_index=None,\\\n",
    "                  reg_E=1, reg_J=1):\n",
    "    \n",
    "    folds, shulffled = train_test_split(df, fold)\n",
    "    pca_rmse_lst = []\n",
    "    cp_rmse_lst = []\n",
    "    xbg_rmse_lst = []\n",
    "    lg_rmse_lst = []\n",
    "    bs_rmse_lst = []\n",
    "    \n",
    "    for i in range(len(folds)):    \n",
    "        train, test = folds[i][0], folds[i][1]\n",
    "        if data == 'wiki' or data == 'tsf' or data == 'tsfmt':\n",
    "            # combined mask masks off entries that are missing in the original dataset and entries in test set\n",
    "            combined_mask, test_missing_entries = mask_out_test_y(test, missing, tensor, index_lst, data)\n",
    "            \n",
    "        if data == 'f1':\n",
    "            combined_mask, test_missing_entries = mask_out_test_y_f1(test, missing, tensor, index_lst)\n",
    "            \n",
    "        if data == 'cws':\n",
    "            combined_mask, test_missing_entries = mask_out_test_y_cws(test, missing, tensor, index_lst)\n",
    "\n",
    "        masked_tensor = tensor * combined_mask\n",
    "        \n",
    "        tensor_pred_pca = []\n",
    "        tensor_pred_cp = []\n",
    "        true_y = []\n",
    "\n",
    "        # (1) Robust-PCA\n",
    "        pca_res = robust_pca(masked_tensor, mask=combined_mask, reg_E=reg_E, reg_J=reg_J)    \n",
    "        # Use only the low rank part, http://jeankossaifi.com/blog/rpca.html\n",
    "        pca_reconstructed = pca_res[0]\n",
    "        \n",
    "        \n",
    "        # (2) CP  \n",
    "        (w, f), err = parafac(masked_tensor, rank=cp_rank, mask = combined_mask, return_errors = True)\n",
    "        cp_reconstructed = tl.kruskal_to_tensor((w, f))\n",
    "        \n",
    "\n",
    "        for m in test_missing_entries:\n",
    "            blank = convert_index(m, index_lst)\n",
    "            pred_pca = read_tensor(pca_reconstructed, blank)\n",
    "            pred_cp = read_tensor(cp_reconstructed, blank)\n",
    "            true = read_tensor(tensor, blank)\n",
    "\n",
    "            # if tensor has been standardized \n",
    "            if scale_back:\n",
    "                if data == 'wiki':\n",
    "                    pred_pca = transform(df.loc[:,'BLEU'], pred_pca)\n",
    "                    pred_cp = transform(df.loc[:,'BLEU'], pred_cp)\n",
    "                    true = transform(df.loc[:,'BLEU'], true)\n",
    "                \n",
    "                if data == 'tsf':\n",
    "                    pred_pca = transform(df.loc[:,'Accuracy'], pred_pca)\n",
    "                    pred_cp = transform(df.loc[:,'Accuracy'], pred_cp)\n",
    "                    true = transform(df.loc[:,'Accuracy'], true)\n",
    "                \n",
    "                if data == 'tsfmt':\n",
    "                    pred_pca = transform(df.loc[:,'BLEU'], pred_pca)\n",
    "                    pred_cp = transform(df.loc[:,'BLEU'], pred_cp)\n",
    "                    true = transform(df.loc[:,'BLEU'], true)\n",
    "                \n",
    "                if data == 'f1':\n",
    "                    _, m, sd = scale_f1(non_scaled_tensor, non_scaled_mode_dict, non_scaled_index)\n",
    "                    pred_pca = scale_back_f1(pred_pca, m, sd)\n",
    "                    pred_cp = scale_back_f1(pred_cp, m, sd)\n",
    "                    true = scale_back_f1(true, m, sd)\n",
    "                    \n",
    "                    \n",
    "            tensor_pred_pca.append(pred_pca)\n",
    "            tensor_pred_cp.append(pred_cp)\n",
    "            true_y.append(true)\n",
    "\n",
    "\n",
    "        # (3) xgboost\n",
    "    \n",
    "        # take in corresponding feature columns\n",
    "        if data == 'wiki' or data == 'tsf' or data == 'tsfmt':\n",
    "            fold_train_X = train.iloc[:,3:]\n",
    "            fold_train_y = train.iloc[:,2]\n",
    "            fold_test_X = test.iloc[:,3:]\n",
    "            fold_test_y = test.iloc[:,2]\n",
    "            \n",
    "        if data == 'f1' or data == 'cws':\n",
    "            train = categorize_df_f1(train, index_lst)\n",
    "            test = categorize_df_f1(test, index_lst)\n",
    "            fold_train_X = train.iloc[:,:-1]\n",
    "            fold_train_y = train.iloc[:,-1]\n",
    "            fold_test_X = test.iloc[:,:-1]\n",
    "            fold_test_y = test.iloc[:,-1]\n",
    "            \n",
    "        \n",
    "        reg = xgb.XGBRegressor(objective ='reg:squarederror', learning_rate=0.1,\n",
    "                                   max_depth=10, n_estimators=100)\n",
    "        res = reg.fit(fold_train_X, fold_train_y)\n",
    "        xgb_pred_y = res.predict(fold_test_X)\n",
    "        \n",
    "        \n",
    "        # (4) LGBM\n",
    "            \n",
    "        lg = lgb.LGBMRegressor(objective='regression')\n",
    "        lg.fit(fold_train_X, fold_train_y, verbose=False)\n",
    "        lg_pred_y = lg.predict(fold_test_X)\n",
    " \n",
    "        \n",
    "        # (5) Baseline\n",
    "        mean_y = np.mean(fold_train_y)\n",
    "        base_pred_y = [mean_y]*len(lg_pred_y)\n",
    "        \n",
    "        # Calculate RMSE\n",
    "        rmse_pca = np.sqrt(mean_squared_error(tensor_pred_pca, true_y))\n",
    "        rmse_cp = np.sqrt(mean_squared_error(tensor_pred_cp, true_y))\n",
    "        rmse_xgb = np.sqrt(mean_squared_error(xgb_pred_y, fold_test_y))\n",
    "        rmse_lg = np.sqrt(mean_squared_error(lg_pred_y, fold_test_y))\n",
    "        rmse_bs = np.sqrt(mean_squared_error(base_pred_y, fold_test_y))\n",
    "        \n",
    "        \n",
    "        pca_rmse_lst.append(rmse_pca)\n",
    "        cp_rmse_lst.append(rmse_cp)\n",
    "        xbg_rmse_lst.append(rmse_xgb)\n",
    "        lg_rmse_lst.append(rmse_lg)\n",
    "        bs_rmse_lst.append(rmse_bs)\n",
    "        \n",
    "        \n",
    "    return pca_rmse_lst, cp_rmse_lst, xbg_rmse_lst, lg_rmse_lst, bs_rmse_lst\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: NER\n",
    "### F1 Tensor Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in a F1 dictionary \n",
    "fileName = \"data/model_data_metric_bucket_evals6.pkl\"\n",
    "dict_f1 = get_dict_f1(fileName)\n",
    "# Order 4 buckets of values in increasing order \n",
    "dict_f1 = order_buckets_f1(dict_f1)\n",
    "\n",
    "\n",
    "# Find out all unique feature names and define the 4 dimensions for a tensor\n",
    "# 4 modes: 0: models, 1: datasets, 2: attributes, 3: buckets\n",
    "mdict = get_modes_f1(dict_f1, 4)\n",
    "\n",
    "\n",
    "# Get a mapping between all unique feature names and tensor indices\n",
    "index_mode = get_index_f1(mdict)\n",
    "\n",
    "# Construct a tensor (for PCA and CP models) \n",
    "# with a mode dict, the original tensor dict, and mapping between mode and indices\n",
    "t1 = construct_tensor_f1(mdict, dict_f1,index_mode)\n",
    "\n",
    "\n",
    "# Convert f1 tensor to a dataframe (for XGboost and LGBM models)\n",
    "f1_df = convert_to_df_f1(t1, mdict, index_mode)\n",
    "\n",
    "# convert columns to integers\n",
    "int_f1_df = categorize_df_f1(f1_df, index_mode)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Performance Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ns_gp_result_f1\n",
    "ns_pca_result_f1, ns_cp_result_f1, ns_xgb_result_f1, ns_lg_result_f1, ns_bs_result_f1 = compare_models(f1_df, t1, [], \\\n",
    "                                                                     index_mode, 5, data='f1',\\\n",
    "                                                                     scale_back=False,\\\n",
    "                                                                     reg_E=1, reg_J=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.052179318"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean RMSE for PCA \n",
    "np.mean(ns_pca_result_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06929777"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean RMSE for CP \n",
    "np.mean(ns_cp_result_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.055889648180819186"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean RMSE for XGBoost \n",
    "np.mean(ns_xgb_result_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.051130917351851636"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean RMSE for LGBM \n",
    "np.mean(ns_lg_result_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20874335857003318"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean RMSE for baseline\n",
    "np.mean(ns_bs_result_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: MT\n",
    "### Wiki Tensor Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in wiki data\n",
    "with open(\"data/dict_file_wiki.pkl\", \"rb\") as f:\n",
    "    wkdata = pickle.load(f)\n",
    "    \n",
    "X = wkdata['BLEU']['feats']\n",
    "y = wkdata['BLEU']['labels']\n",
    "langs = wkdata['BLEU']['langs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain a wiki dataframe\n",
    "wiki = pd.concat([langs, y, X], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a tensor with 3 modes (source lang, target lang, features)\n",
    "tensor, missing, mode_dict, ind_lst, all_p = construct_tensor_from_df(wiki, lang_modes=[0,1], \\\n",
    "                                           feat_mode=list(range(2,len(wiki.columns))), \\\n",
    "                                           need_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3458251150558843"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sparcity\n",
    "len(missing)/tensor.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wikiMT Performance Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_result, cp_result, xgb_result, lg_result, bs_result = compare_models(wiki, tensor, missing, ind_lst, 5, reg_E=0.6, reg_J=1.65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.941274638049036"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean RMSE for PCA \n",
    "np.mean(pca_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.076549853475644"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean RMSE for CP \n",
    "np.mean(cp_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.366065267157411"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean RMSE for XGBoost \n",
    "np.mean(xgb_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.941274638049036"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean RMSE for PCA \n",
    "np.mean(pca_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2561946304899245"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean RMSE for LGBM \n",
    "np.mean(lg_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.389119363990967"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean RMSE for baseline\n",
    "np.mean(bs_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: POS\n",
    "### tsf Tensor Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in tsf data\n",
    "with open(\"data/dict_file_tsf.pkl\", \"rb\") as f:\n",
    "    tsfdata = pickle.load(f)\n",
    "    \n",
    "X_tsf = tsfdata['Accuracy']['feats']\n",
    "y_tsf = tsfdata['Accuracy']['labels']\n",
    "langs_tsf = tsfdata['Accuracy']['langs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain a dataframe\n",
    "tsf = pd.concat([langs_tsf, y_tsf, X_tsf], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a tensor with 3 modes\n",
    "tensor_tsf, missing_tsf, mode_dict_tsf, ind_lst_tsf, all_p_tsf = construct_tensor_from_df(tsf, lang_modes=[0,1], \\\n",
    "                                           feat_mode=list(range(2,len(tsf.columns))),\\\n",
    "                                           need_scale=False, source='Task lang', target='Aux lang')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01858974358974359"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sparcity\n",
    "len(missing_tsf)/tensor_tsf.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tsfPOS Performance Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_result_tsf, cp_result_tsf, xgb_result_tsf,lg_result_tsf, bs_result_tsf = compare_models(tsf, tensor_tsf, missing_tsf, ind_lst_tsf, 5, \\\n",
    "                                                              data='tsf', scale_back=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.818490958859231"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean RMSE for PCA \n",
    "np.mean(pca_result_tsf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34.84026188746132"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean RMSE for CP\n",
    "np.mean(cp_result_tsf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.278306714645728"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean RMSE for XGBoost \n",
    "np.mean(xgb_result_tsf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.245376756573846"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean RMSE for LGBM \n",
    "np.mean(lg_result_tsf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.097613010215706"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean RMSE for Baseline \n",
    "np.mean(bs_result_tsf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: CWS\n",
    "### Tensor Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in a dictionary \n",
    "true_cws =  \"data/dict_file_cws.pkl\"\n",
    "with open(true_cws, \"rb\") as f:\n",
    "    cws_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "cws_modes = get_modes_f1(cws_dict, 4)\n",
    "cws_index = get_index_f1(cws_modes)\n",
    "cws_t = construct_tensor_f1(cws_modes, cws_dict, cws_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert tensor to a dataframe\n",
    "cws_df = convert_to_df_cws(cws_t, cws_modes, cws_index)\n",
    "\n",
    "# convert columns to integers\n",
    "int_cws_df = categorize_df_cws(cws_df, cws_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_result_cws, cp_result_cws, xgb_result_cws,lg_result_cws, bs_result_cws = compare_models(cws_df, cws_t, [], \\\n",
    "                                                                     cws_index, 5, data='cws',\\\n",
    "                                                                     scale_back=False,\\\n",
    "                                                                     reg_E=1, reg_J=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02788474"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean RMSE for PCA \n",
    "np.mean(pca_result_cws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04636464"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean RMSE for CP \n",
    "np.mean(cp_result_cws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.017748032775031924"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean RMSE for XGBoost \n",
    "np.mean(xgb_result_cws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03557505761049142"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean RMSE for LGBM \n",
    "np.mean(lg_result_cws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13689612466730292"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean RMSE for Baseline \n",
    "np.mean(bs_result_cws)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
